{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.26.4'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientDescent(X,y,epochs,learning_rate,tol):\n",
    "    \n",
    "    n_samples, n_features = X.shape\n",
    "    weights = np.zeros(n_features)\n",
    "    bias = 0\n",
    "\n",
    "    print(f\"Epoch: -1 | MAE Train Loss: None current Weight -> {weights} ,current bias -> {bias} \")\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        y_predicted = np.dot(weights,X.T) + bias\n",
    "        \n",
    "        error = y_predicted[0] - y\n",
    "        loss = np.mean((y - y_predicted)**2)\n",
    "\n",
    "        gradient_weights = np.dot(X.T, error) / X.shape[0]\n",
    "        gradient_bias = np.mean(error)\n",
    "\n",
    "        weights = weights - learning_rate * gradient_weights\n",
    "        bias = bias - learning_rate * gradient_bias\n",
    "\n",
    "        print(f\"Epoch: {epoch} | MAE Train Loss: {loss} current Weight -> {weights} ,current bias -> {bias} \")\n",
    "\n",
    "        if np.linalg.norm(gradient_weights) < tol:\n",
    "            print(\"Convergence reached.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Known Weights and bias\n",
    "w = np.array([2,4])\n",
    "b = 2\n",
    "\n",
    "# Dataset\n",
    "X = np.random.randn(100, 2)\n",
    "y = np.dot(X,w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: -1 | MAE Train Loss: None current Weight -> [0. 0.] ,current bias -> 0 \n",
      "Epoch: 0 | MAE Train Loss: 26.489855254463468 current Weight -> [0.002812   0.00434465] ,current bias -> 0.0017436297734465622 \n",
      "Epoch: 1 | MAE Train Loss: 26.430242377652206 current Weight -> [0.005624  0.0086894] ,current bias -> 0.003485563690428734 \n",
      "Epoch: 2 | MAE Train Loss: 26.37070294706096 current Weight -> [0.00843602 0.01303426] ,current bias -> 0.005225803504097269 \n",
      "Epoch: 3 | MAE Train Loss: 26.31123694589066 current Weight -> [0.01124804 0.01737923] ,current bias -> 0.006964350965790543 \n",
      "Epoch: 4 | MAE Train Loss: 26.251844357381056 current Weight -> [0.01406006 0.0217243 ] ,current bias -> 0.008701207825036424 \n",
      "Epoch: 5 | MAE Train Loss: 26.192525164810608 current Weight -> [0.0168721  0.02606947] ,current bias -> 0.010436375829554142 \n",
      "Epoch: 6 | MAE Train Loss: 26.1332793514964 current Weight -> [0.01968414 0.03041475] ,current bias -> 0.012169856725256162 \n",
      "Epoch: 7 | MAE Train Loss: 26.07410690079408 current Weight -> [0.02249618 0.03476014] ,current bias -> 0.013901652256250055 \n",
      "Epoch: 8 | MAE Train Loss: 26.015007796097724 current Weight -> [0.02530824 0.03910563] ,current bias -> 0.015631764164840356 \n",
      "Epoch: 9 | MAE Train Loss: 25.95598202083982 current Weight -> [0.0281203  0.04345122] ,current bias -> 0.017360194191530433 \n",
      "Epoch: 10 | MAE Train Loss: 25.897029558491095 current Weight -> [0.03093236 0.04779692] ,current bias -> 0.019086944075024346 \n",
      "Epoch: 11 | MAE Train Loss: 25.838150392560532 current Weight -> [0.03374444 0.05214272] ,current bias -> 0.02081201555222871 \n",
      "Epoch: 12 | MAE Train Loss: 25.779344506595194 current Weight -> [0.03655652 0.05648862] ,current bias -> 0.022535410358254557 \n",
      "Epoch: 13 | MAE Train Loss: 25.720611884180176 current Weight -> [0.0393686  0.06083463] ,current bias -> 0.024257130226419184 \n",
      "Epoch: 14 | MAE Train Loss: 25.661952508938544 current Weight -> [0.0421807  0.06518075] ,current bias -> 0.025977176888248008 \n",
      "Epoch: 15 | MAE Train Loss: 25.603366364531222 current Weight -> [0.0449928  0.06952696] ,current bias -> 0.027695552073476425 \n",
      "Epoch: 16 | MAE Train Loss: 25.544853434656897 current Weight -> [0.04780491 0.07387329] ,current bias -> 0.029412257510051654 \n",
      "Epoch: 17 | MAE Train Loss: 25.486413703051966 current Weight -> [0.05061702 0.07821971] ,current bias -> 0.03112729492413459 \n",
      "Epoch: 18 | MAE Train Loss: 25.428047153490454 current Weight -> [0.05342914 0.08256624] ,current bias -> 0.032840666040101656 \n",
      "Epoch: 19 | MAE Train Loss: 25.36975376978386 current Weight -> [0.05624127 0.08691287] ,current bias -> 0.03455237258054662 \n",
      "Epoch: 20 | MAE Train Loss: 25.311533535781187 current Weight -> [0.0590534 0.0912596] ,current bias -> 0.03626241626628249 \n",
      "Epoch: 21 | MAE Train Loss: 25.25338643536877 current Weight -> [0.06186554 0.09560644] ,current bias -> 0.037970798816343285 \n",
      "Epoch: 22 | MAE Train Loss: 25.19531245247023 current Weight -> [0.06467769 0.09995338] ,current bias -> 0.03967752194798595 \n",
      "Epoch: 23 | MAE Train Loss: 25.137311571046368 current Weight -> [0.06748984 0.10430043] ,current bias -> 0.04138258737669213 \n",
      "Epoch: 24 | MAE Train Loss: 25.07938377509511 current Weight -> [0.070302   0.10864757] ,current bias -> 0.043085996816170036 \n",
      "Epoch: 25 | MAE Train Loss: 25.021529048651427 current Weight -> [0.07311417 0.11299482] ,current bias -> 0.044787751978356286 \n",
      "Epoch: 26 | MAE Train Loss: 24.963747375787207 current Weight -> [0.07592635 0.11734217] ,current bias -> 0.04648785457341771 \n",
      "Epoch: 27 | MAE Train Loss: 24.90603874061122 current Weight -> [0.07873853 0.12168963] ,current bias -> 0.04818630630975321 \n",
      "Epoch: 28 | MAE Train Loss: 24.848403127269012 current Weight -> [0.08155071 0.12603718] ,current bias -> 0.04988310889399555 \n",
      "Epoch: 29 | MAE Train Loss: 24.79084051994284 current Weight -> [0.08436291 0.13038484] ,current bias -> 0.05157826403101321 \n",
      "Epoch: 30 | MAE Train Loss: 24.733350902851562 current Weight -> [0.08717511 0.1347326 ] ,current bias -> 0.05327177342391221 \n",
      "Epoch: 31 | MAE Train Loss: 24.675934260250614 current Weight -> [0.08998732 0.13908047] ,current bias -> 0.054963638774037926 \n",
      "Epoch: 32 | MAE Train Loss: 24.61859057643184 current Weight -> [0.09279953 0.14342843] ,current bias -> 0.056653861780976894 \n",
      "Epoch: 33 | MAE Train Loss: 24.561319835723474 current Weight -> [0.09561175 0.1477765 ] ,current bias -> 0.05834244414255865 \n",
      "Epoch: 34 | MAE Train Loss: 24.50412202249007 current Weight -> [0.09842398 0.15212467] ,current bias -> 0.06002938755485755 \n",
      "Epoch: 35 | MAE Train Loss: 24.446997121132355 current Weight -> [0.10123621 0.15647294] ,current bias -> 0.06171469371219457 \n",
      "Epoch: 36 | MAE Train Loss: 24.389945116087222 current Weight -> [0.10404845 0.16082131] ,current bias -> 0.0633983643071391 \n",
      "Epoch: 37 | MAE Train Loss: 24.33296599182761 current Weight -> [0.1068607  0.16516978] ,current bias -> 0.06508040103051081 \n",
      "Epoch: 38 | MAE Train Loss: 24.276059732862404 current Weight -> [0.10967295 0.16951836] ,current bias -> 0.0667608055713814 \n",
      "Epoch: 39 | MAE Train Loss: 24.21922632373642 current Weight -> [0.11248521 0.17386703] ,current bias -> 0.06843957961707647 \n",
      "Epoch: 40 | MAE Train Loss: 24.162465749030254 current Weight -> [0.11529748 0.17821581] ,current bias -> 0.07011672485317724 \n",
      "Epoch: 41 | MAE Train Loss: 24.105777993360256 current Weight -> [0.11810975 0.18256469] ,current bias -> 0.07179224296352243 \n",
      "Epoch: 42 | MAE Train Loss: 24.049163041378414 current Weight -> [0.12092203 0.18691367] ,current bias -> 0.07346613563021004 \n",
      "Epoch: 43 | MAE Train Loss: 23.992620877772307 current Weight -> [0.12373432 0.19126274] ,current bias -> 0.07513840453359912 \n",
      "Epoch: 44 | MAE Train Loss: 23.93615148726497 current Weight -> [0.12654661 0.19561193] ,current bias -> 0.07680905135231159 \n",
      "Epoch: 45 | MAE Train Loss: 23.879754854614898 current Weight -> [0.12935891 0.19996121] ,current bias -> 0.07847807776323405 \n",
      "Epoch: 46 | MAE Train Loss: 23.823430964615888 current Weight -> [0.13217121 0.20431059] ,current bias -> 0.08014548544151957 \n",
      "Epoch: 47 | MAE Train Loss: 23.767179802097008 current Weight -> [0.13498353 0.20866007] ,current bias -> 0.08181127606058947 \n",
      "Epoch: 48 | MAE Train Loss: 23.711001351922523 current Weight -> [0.13779584 0.21300965] ,current bias -> 0.08347545129213507 \n",
      "Epoch: 49 | MAE Train Loss: 23.65489559899177 current Weight -> [0.14060817 0.21735933] ,current bias -> 0.08513801280611957 \n",
      "Epoch: 50 | MAE Train Loss: 23.59886252823911 current Weight -> [0.1434205  0.22170912] ,current bias -> 0.08679896227077977 \n",
      "Epoch: 51 | MAE Train Loss: 23.542902124633855 current Weight -> [0.14623284 0.226059  ] ,current bias -> 0.08845830135262786 \n",
      "Epoch: 52 | MAE Train Loss: 23.487014373180187 current Weight -> [0.14904518 0.23040898] ,current bias -> 0.09011603171645324 \n",
      "Epoch: 53 | MAE Train Loss: 23.431199258917072 current Weight -> [0.15185753 0.23475906] ,current bias -> 0.09177215502532426 \n",
      "Epoch: 54 | MAE Train Loss: 23.375456766918177 current Weight -> [0.15466989 0.23910924] ,current bias -> 0.09342667294059002 \n",
      "Epoch: 55 | MAE Train Loss: 23.319786882291822 current Weight -> [0.15748226 0.24345952] ,current bias -> 0.09507958712188214 \n",
      "Epoch: 56 | MAE Train Loss: 23.264189590180866 current Weight -> [0.16029463 0.24780991] ,current bias -> 0.09673089922711653 \n",
      "Epoch: 57 | MAE Train Loss: 23.208664875762658 current Weight -> [0.163107   0.25216039] ,current bias -> 0.09838061091249517 \n",
      "Epoch: 58 | MAE Train Loss: 23.15321272424894 current Weight -> [0.16591939 0.25651096] ,current bias -> 0.10002872383250791 \n",
      "Epoch: 59 | MAE Train Loss: 23.097833120885774 current Weight -> [0.16873178 0.26086164] ,current bias -> 0.10167523963993419 \n",
      "Epoch: 60 | MAE Train Loss: 23.042526050953494 current Weight -> [0.17154417 0.26521242] ,current bias -> 0.10332015998584484 \n",
      "Epoch: 61 | MAE Train Loss: 22.987291499766574 current Weight -> [0.17435657 0.2695633 ] ,current bias -> 0.10496348651960384 \n",
      "Epoch: 62 | MAE Train Loss: 22.93212945267362 current Weight -> [0.17716898 0.27391427] ,current bias -> 0.10660522088887008 \n",
      "Epoch: 63 | MAE Train Loss: 22.877039895057226 current Weight -> [0.1799814  0.27826535] ,current bias -> 0.1082453647395991 \n",
      "Epoch: 64 | MAE Train Loss: 22.822022812333948 current Weight -> [0.18279382 0.28261652] ,current bias -> 0.10988391971604493 \n",
      "Epoch: 65 | MAE Train Loss: 22.7670781899542 current Weight -> [0.18560625 0.28696779] ,current bias -> 0.11152088746076175 \n",
      "Epoch: 66 | MAE Train Loss: 22.712206013402195 current Weight -> [0.18841868 0.29131916] ,current bias -> 0.1131562696146057 \n",
      "Epoch: 67 | MAE Train Loss: 22.657406268195867 current Weight -> [0.19123112 0.29567063] ,current bias -> 0.11479006781673666 \n",
      "Epoch: 68 | MAE Train Loss: 22.60267893988678 current Weight -> [0.19404357 0.30002219] ,current bias -> 0.11642228370461992 \n",
      "Epoch: 69 | MAE Train Loss: 22.54802401406009 current Weight -> [0.19685602 0.30437386] ,current bias -> 0.11805291891402801 \n",
      "Epoch: 70 | MAE Train Loss: 22.493441476334393 current Weight -> [0.19966848 0.30872562] ,current bias -> 0.11968197507904244 \n",
      "Epoch: 71 | MAE Train Loss: 22.438931312361756 current Weight -> [0.20248095 0.31307748] ,current bias -> 0.1213094538320554 \n",
      "Epoch: 72 | MAE Train Loss: 22.38449350782757 current Weight -> [0.20529342 0.31742944] ,current bias -> 0.12293535680377156 \n",
      "Epoch: 73 | MAE Train Loss: 22.330128048450494 current Weight -> [0.2081059  0.32178149] ,current bias -> 0.12455968562320978 \n",
      "Epoch: 74 | MAE Train Loss: 22.27583491998236 current Weight -> [0.21091839 0.32613365] ,current bias -> 0.12618244191770486 \n",
      "Epoch: 75 | MAE Train Loss: 22.221614108208154 current Weight -> [0.21373088 0.3304859 ] ,current bias -> 0.1278036273129093 \n",
      "Epoch: 76 | MAE Train Loss: 22.1674655989459 current Weight -> [0.21654338 0.33483825] ,current bias -> 0.12942324343279504 \n",
      "Epoch: 77 | MAE Train Loss: 22.11338937804658 current Weight -> [0.21935588 0.33919069] ,current bias -> 0.13104129189965513 \n",
      "Epoch: 78 | MAE Train Loss: 22.05938543139407 current Weight -> [0.22216839 0.34354323] ,current bias -> 0.13265777433410555 \n",
      "Epoch: 79 | MAE Train Loss: 22.005453744905097 current Weight -> [0.22498091 0.34789587] ,current bias -> 0.13427269235508693 \n",
      "Epoch: 80 | MAE Train Loss: 21.951594304529117 current Weight -> [0.22779343 0.35224861] ,current bias -> 0.13588604757986625 \n",
      "Epoch: 81 | MAE Train Loss: 21.897807096248272 current Weight -> [0.23060596 0.35660145] ,current bias -> 0.13749784162403855 \n",
      "Epoch: 82 | MAE Train Loss: 21.84409210607731 current Weight -> [0.2334185  0.36095438] ,current bias -> 0.13910807610152875 \n",
      "Epoch: 83 | MAE Train Loss: 21.790449320063516 current Weight -> [0.23623104 0.3653074 ] ,current bias -> 0.1407167526245933 \n",
      "Epoch: 84 | MAE Train Loss: 21.736878724286612 current Weight -> [0.23904359 0.36966053] ,current bias -> 0.14232387280382186 \n",
      "Epoch: 85 | MAE Train Loss: 21.68338030485875 current Weight -> [0.24185614 0.37401375] ,current bias -> 0.1439294382481392 \n",
      "Epoch: 86 | MAE Train Loss: 21.629954047924347 current Weight -> [0.2446687  0.37836707] ,current bias -> 0.1455334505648067 \n",
      "Epoch: 87 | MAE Train Loss: 21.57659993966011 current Weight -> [0.24748127 0.38272048] ,current bias -> 0.14713591135942428 \n",
      "Epoch: 88 | MAE Train Loss: 21.523317966274885 current Weight -> [0.25029384 0.38707399] ,current bias -> 0.1487368222359319 \n",
      "Epoch: 89 | MAE Train Loss: 21.470108114009623 current Weight -> [0.25310642 0.39142759] ,current bias -> 0.15033618479661148 \n",
      "Epoch: 90 | MAE Train Loss: 21.416970369137307 current Weight -> [0.25591901 0.3957813 ] ,current bias -> 0.1519340006420885 \n",
      "Epoch: 91 | MAE Train Loss: 21.363904717962868 current Weight -> [0.2587316  0.40013509] ,current bias -> 0.15353027137133374 \n",
      "Epoch: 92 | MAE Train Loss: 21.310911146823145 current Weight -> [0.2615442  0.40448899] ,current bias -> 0.15512499858166498 \n",
      "Epoch: 93 | MAE Train Loss: 21.257989642086756 current Weight -> [0.2643568  0.40884298] ,current bias -> 0.15671818386874872 \n",
      "Epoch: 94 | MAE Train Loss: 21.205140190154076 current Weight -> [0.26716941 0.41319706] ,current bias -> 0.15830982882660188 \n",
      "Epoch: 95 | MAE Train Loss: 21.152362777457153 current Weight -> [0.26998203 0.41755124] ,current bias -> 0.1598999350475935 \n",
      "Epoch: 96 | MAE Train Loss: 21.09965739045963 current Weight -> [0.27279465 0.42190552] ,current bias -> 0.1614885041224465 \n",
      "Epoch: 97 | MAE Train Loss: 21.04702401565669 current Weight -> [0.27560728 0.42625989] ,current bias -> 0.16307553764023927 \n",
      "Epoch: 98 | MAE Train Loss: 20.99446263957496 current Weight -> [0.27841992 0.43061435] ,current bias -> 0.1646610371884075 \n",
      "Epoch: 99 | MAE Train Loss: 20.941973248772467 current Weight -> [0.28123256 0.43496892] ,current bias -> 0.16624500435274575 \n",
      "Epoch: 100 | MAE Train Loss: 20.889555829838542 current Weight -> [0.2840452  0.43932357] ,current bias -> 0.16782744071740924 \n",
      "Epoch: 101 | MAE Train Loss: 20.837210369393784 current Weight -> [0.28685786 0.44367832] ,current bias -> 0.16940834786491552 \n",
      "Epoch: 102 | MAE Train Loss: 20.784936854089953 current Weight -> [0.28967052 0.44803317] ,current bias -> 0.17098772737614615 \n",
      "Epoch: 103 | MAE Train Loss: 20.73273527060992 current Weight -> [0.29248318 0.45238811] ,current bias -> 0.1725655808303484 \n",
      "Epoch: 104 | MAE Train Loss: 20.680605605667598 current Weight -> [0.29529586 0.45674315] ,current bias -> 0.17414190980513694 \n",
      "Epoch: 105 | MAE Train Loss: 20.628547846007883 current Weight -> [0.29810853 0.46109828] ,current bias -> 0.1757167158764955 \n",
      "Epoch: 106 | MAE Train Loss: 20.576561978406545 current Weight -> [0.30092122 0.4654535 ] ,current bias -> 0.17729000061877867 \n",
      "Epoch: 107 | MAE Train Loss: 20.524647989670203 current Weight -> [0.30373391 0.46980882] ,current bias -> 0.17886176560471337 \n",
      "Epoch: 108 | MAE Train Loss: 20.47280586663623 current Weight -> [0.30654661 0.47416423] ,current bias -> 0.18043201240540072 \n",
      "Epoch: 109 | MAE Train Loss: 20.421035596172693 current Weight -> [0.30935931 0.47851974] ,current bias -> 0.18200074259031768 \n",
      "Epoch: 110 | MAE Train Loss: 20.369337165178294 current Weight -> [0.31217202 0.48287534] ,current bias -> 0.18356795772731868 \n",
      "Epoch: 111 | MAE Train Loss: 20.317710560582274 current Weight -> [0.31498473 0.48723104] ,current bias -> 0.18513365938263734 \n",
      "Epoch: 112 | MAE Train Loss: 20.266155769344373 current Weight -> [0.31779745 0.49158683] ,current bias -> 0.18669784912088813 \n",
      "Epoch: 113 | MAE Train Loss: 20.214672778454744 current Weight -> [0.32061018 0.49594271] ,current bias -> 0.18826052850506803 \n",
      "Epoch: 114 | MAE Train Loss: 20.163261574933905 current Weight -> [0.32342291 0.50029869] ,current bias -> 0.18982169909655822 \n",
      "Epoch: 115 | MAE Train Loss: 20.11192214583263 current Weight -> [0.32623565 0.50465476] ,current bias -> 0.19138136245512574 \n",
      "Epoch: 116 | MAE Train Loss: 20.060654478231935 current Weight -> [0.3290484  0.50901092] ,current bias -> 0.1929395201389252 \n",
      "Epoch: 117 | MAE Train Loss: 20.009458559242972 current Weight -> [0.33186115 0.51336718] ,current bias -> 0.19449617370450042 \n",
      "Epoch: 118 | MAE Train Loss: 19.958334376006967 current Weight -> [0.33467391 0.51772353] ,current bias -> 0.19605132470678605 \n",
      "Epoch: 119 | MAE Train Loss: 19.907281915695176 current Weight -> [0.33748667 0.52207997] ,current bias -> 0.19760497469910926 \n",
      "Epoch: 120 | MAE Train Loss: 19.85630116550879 current Weight -> [0.34029944 0.52643651] ,current bias -> 0.19915712523319146 \n",
      "Epoch: 121 | MAE Train Loss: 19.805392112678877 current Weight -> [0.34311221 0.53079314] ,current bias -> 0.20070777785914987 \n",
      "Epoch: 122 | MAE Train Loss: 19.754554744466315 current Weight -> [0.34592499 0.53514986] ,current bias -> 0.20225693412549928 \n",
      "Epoch: 123 | MAE Train Loss: 19.703789048161738 current Weight -> [0.34873778 0.53950668] ,current bias -> 0.2038045955791536 \n",
      "Epoch: 124 | MAE Train Loss: 19.653095011085455 current Weight -> [0.35155058 0.54386358] ,current bias -> 0.20535076376542755 \n",
      "Epoch: 125 | MAE Train Loss: 19.60247262058737 current Weight -> [0.35436337 0.54822058] ,current bias -> 0.2068954402280384 \n",
      "Epoch: 126 | MAE Train Loss: 19.551921864046957 current Weight -> [0.35717618 0.55257768] ,current bias -> 0.2084386265091075 \n",
      "Epoch: 127 | MAE Train Loss: 19.50144272887315 current Weight -> [0.35998899 0.55693486] ,current bias -> 0.20998032414916193 \n",
      "Epoch: 128 | MAE Train Loss: 19.451035202504308 current Weight -> [0.36280181 0.56129214] ,current bias -> 0.2115205346871363 \n",
      "Epoch: 129 | MAE Train Loss: 19.400699272408136 current Weight -> [0.36561463 0.56564951] ,current bias -> 0.21305925966037423 \n",
      "Epoch: 130 | MAE Train Loss: 19.350434926081608 current Weight -> [0.36842746 0.57000697] ,current bias -> 0.21459650060463006 \n",
      "Epoch: 131 | MAE Train Loss: 19.300242151050934 current Weight -> [0.3712403  0.57436453] ,current bias -> 0.21613225905407052 \n",
      "Epoch: 132 | MAE Train Loss: 19.250120934871457 current Weight -> [0.37405314 0.57872217] ,current bias -> 0.2176665365412763 \n",
      "Epoch: 133 | MAE Train Loss: 19.200071265127615 current Weight -> [0.37686599 0.58307991] ,current bias -> 0.21919933459724372 \n",
      "Epoch: 134 | MAE Train Loss: 19.15009312943286 current Weight -> [0.37967884 0.58743774] ,current bias -> 0.22073065475138645 \n",
      "Epoch: 135 | MAE Train Loss: 19.1001865154296 current Weight -> [0.3824917  0.59179566] ,current bias -> 0.22226049853153704 \n",
      "Epoch: 136 | MAE Train Loss: 19.050351410789133 current Weight -> [0.38530456 0.59615368] ,current bias -> 0.22378886746394858 \n",
      "Epoch: 137 | MAE Train Loss: 19.000587803211598 current Weight -> [0.38811743 0.60051178] ,current bias -> 0.22531576307329634 \n",
      "Epoch: 138 | MAE Train Loss: 18.95089568042586 current Weight -> [0.39093031 0.60486998] ,current bias -> 0.22684118688267943 \n",
      "Epoch: 139 | MAE Train Loss: 18.901275030189506 current Weight -> [0.39374319 0.60922826] ,current bias -> 0.22836514041362235 \n",
      "Epoch: 140 | MAE Train Loss: 18.851725840288747 current Weight -> [0.39655608 0.61358664] ,current bias -> 0.22988762518607675 \n",
      "Epoch: 141 | MAE Train Loss: 18.802248098538367 current Weight -> [0.39936898 0.61794511] ,current bias -> 0.2314086427184229 \n",
      "Epoch: 142 | MAE Train Loss: 18.752841792781645 current Weight -> [0.40218188 0.62230367] ,current bias -> 0.23292819452747146 \n",
      "Epoch: 143 | MAE Train Loss: 18.70350691089031 current Weight -> [0.40499478 0.62666232] ,current bias -> 0.23444628212846497 \n",
      "Epoch: 144 | MAE Train Loss: 18.654243440764443 current Weight -> [0.4078077  0.63102107] ,current bias -> 0.23596290703507955 \n",
      "Epoch: 145 | MAE Train Loss: 18.605051370332465 current Weight -> [0.41062061 0.6353799 ] ,current bias -> 0.23747807075942656 \n",
      "Epoch: 146 | MAE Train Loss: 18.55593068755103 current Weight -> [0.41343354 0.63973882] ,current bias -> 0.2389917748120541 \n",
      "Epoch: 147 | MAE Train Loss: 18.506881380404987 current Weight -> [0.41624647 0.64409784] ,current bias -> 0.2405040207019487 \n",
      "Epoch: 148 | MAE Train Loss: 18.457903436907294 current Weight -> [0.4190594  0.64845694] ,current bias -> 0.24201480993653696 \n",
      "Epoch: 149 | MAE Train Loss: 18.40899684509897 current Weight -> [0.42187235 0.65281613] ,current bias -> 0.24352414402168707 \n",
      "Epoch: 150 | MAE Train Loss: 18.360161593049046 current Weight -> [0.42468529 0.65717542] ,current bias -> 0.2450320244617105 \n",
      "Epoch: 151 | MAE Train Loss: 18.31139766885446 current Weight -> [0.42749825 0.66153479] ,current bias -> 0.2465384527593636 \n",
      "Epoch: 152 | MAE Train Loss: 18.26270506064004 current Weight -> [0.43031121 0.66589426] ,current bias -> 0.24804343041584917 \n",
      "Epoch: 153 | MAE Train Loss: 18.21408375655842 current Weight -> [0.43312417 0.67025382] ,current bias -> 0.24954695893081813 \n",
      "Epoch: 154 | MAE Train Loss: 18.165533744789958 current Weight -> [0.43593714 0.67461346] ,current bias -> 0.25104903980237103 \n",
      "Epoch: 155 | MAE Train Loss: 18.117055013542725 current Weight -> [0.43875012 0.67897319] ,current bias -> 0.25254967452705973 \n",
      "Epoch: 156 | MAE Train Loss: 18.0686475510524 current Weight -> [0.4415631  0.68333302] ,current bias -> 0.254048864599889 \n",
      "Epoch: 157 | MAE Train Loss: 18.020311345582208 current Weight -> [0.44437609 0.68769293] ,current bias -> 0.25554661151431807 \n",
      "Epoch: 158 | MAE Train Loss: 17.972046385422892 current Weight -> [0.44718908 0.69205294] ,current bias -> 0.25704291676226226 \n",
      "Epoch: 159 | MAE Train Loss: 17.923852658892613 current Weight -> [0.45000208 0.69641303] ,current bias -> 0.25853778183409454 \n",
      "Epoch: 160 | MAE Train Loss: 17.875730154336917 current Weight -> [0.45281509 0.70077321] ,current bias -> 0.2600312082186472 \n",
      "Epoch: 161 | MAE Train Loss: 17.827678860128653 current Weight -> [0.4556281  0.70513348] ,current bias -> 0.26152319740321345 \n",
      "Epoch: 162 | MAE Train Loss: 17.77969876466793 current Weight -> [0.45844112 0.70949384] ,current bias -> 0.2630137508735488 \n",
      "Epoch: 163 | MAE Train Loss: 17.731789856382044 current Weight -> [0.46125414 0.71385429] ,current bias -> 0.2645028701138729 \n",
      "Epoch: 164 | MAE Train Loss: 17.683952123725405 current Weight -> [0.46406717 0.71821483] ,current bias -> 0.2659905566068711 \n",
      "Epoch: 165 | MAE Train Loss: 17.636185555179512 current Weight -> [0.4668802  0.72257546] ,current bias -> 0.26747681183369587 \n",
      "Epoch: 166 | MAE Train Loss: 17.58849013925286 current Weight -> [0.46969324 0.72693618] ,current bias -> 0.2689616372739685 \n",
      "Epoch: 167 | MAE Train Loss: 17.540865864480892 current Weight -> [0.47250629 0.73129698] ,current bias -> 0.2704450344057807 \n",
      "Epoch: 168 | MAE Train Loss: 17.493312719425926 current Weight -> [0.47531934 0.73565787] ,current bias -> 0.2719270047056962 \n",
      "Epoch: 169 | MAE Train Loss: 17.445830692677113 current Weight -> [0.4781324  0.74001886] ,current bias -> 0.2734075496487522 \n",
      "Epoch: 170 | MAE Train Loss: 17.398419772850385 current Weight -> [0.48094546 0.74437993] ,current bias -> 0.27488667070846107 \n",
      "Epoch: 171 | MAE Train Loss: 17.35107994858835 current Weight -> [0.48375853 0.74874109] ,current bias -> 0.27636436935681186 \n",
      "Epoch: 172 | MAE Train Loss: 17.303811208560273 current Weight -> [0.48657161 0.75310233] ,current bias -> 0.2778406470642719 \n",
      "Epoch: 173 | MAE Train Loss: 17.256613541462013 current Weight -> [0.48938469 0.75746367] ,current bias -> 0.2793155052997884 \n",
      "Epoch: 174 | MAE Train Loss: 17.20948693601594 current Weight -> [0.49219777 0.76182509] ,current bias -> 0.2807889455307899 \n",
      "Epoch: 175 | MAE Train Loss: 17.162431380970883 current Weight -> [0.49501086 0.7661866 ] ,current bias -> 0.28226096922318816 \n",
      "Epoch: 176 | MAE Train Loss: 17.1154468651021 current Weight -> [0.49782396 0.7705482 ] ,current bias -> 0.28373157784137926 \n",
      "Epoch: 177 | MAE Train Loss: 17.068533377211182 current Weight -> [0.50063706 0.77490989] ,current bias -> 0.2852007728482455 \n",
      "Epoch: 178 | MAE Train Loss: 17.021690906126004 current Weight -> [0.50345017 0.77927166] ,current bias -> 0.286668555705157 \n",
      "Epoch: 179 | MAE Train Loss: 16.97491944070067 current Weight -> [0.50626329 0.78363352] ,current bias -> 0.2881349278719728 \n",
      "Epoch: 180 | MAE Train Loss: 16.928218969815465 current Weight -> [0.50907641 0.78799547] ,current bias -> 0.2895998908070432 \n",
      "Epoch: 181 | MAE Train Loss: 16.88158948237676 current Weight -> [0.51188953 0.79235751] ,current bias -> 0.2910634459672105 \n",
      "Epoch: 182 | MAE Train Loss: 16.835030967317 current Weight -> [0.51470266 0.79671963] ,current bias -> 0.2925255948078112 \n",
      "Epoch: 183 | MAE Train Loss: 16.78854341359462 current Weight -> [0.5175158  0.80108184] ,current bias -> 0.2939863387826771 \n",
      "Epoch: 184 | MAE Train Loss: 16.742126810193973 current Weight -> [0.52032894 0.80544414] ,current bias -> 0.2954456793441372 \n",
      "Epoch: 185 | MAE Train Loss: 16.695781146125306 current Weight -> [0.52314209 0.80980653] ,current bias -> 0.296903617943019 \n",
      "Epoch: 186 | MAE Train Loss: 16.649506410424678 current Weight -> [0.52595525 0.814169  ] ,current bias -> 0.2983601560286503 \n",
      "Epoch: 187 | MAE Train Loss: 16.6033025921539 current Weight -> [0.52876841 0.81853156] ,current bias -> 0.2998152950488604 \n",
      "Epoch: 188 | MAE Train Loss: 16.557169680400488 current Weight -> [0.53158157 0.82289421] ,current bias -> 0.30126903644998204 \n",
      "Epoch: 189 | MAE Train Loss: 16.51110766427761 current Weight -> [0.53439474 0.82725694] ,current bias -> 0.30272138167685264 \n",
      "Epoch: 190 | MAE Train Loss: 16.46511653292401 current Weight -> [0.53720792 0.83161976] ,current bias -> 0.30417233217281603 \n",
      "Epoch: 191 | MAE Train Loss: 16.419196275503975 current Weight -> [0.5400211  0.83598266] ,current bias -> 0.3056218893797239 \n",
      "Epoch: 192 | MAE Train Loss: 16.373346881207244 current Weight -> [0.54283429 0.84034566] ,current bias -> 0.3070700547379374 \n",
      "Epoch: 193 | MAE Train Loss: 16.327568339248977 current Weight -> [0.54564748 0.84470873] ,current bias -> 0.3085168296863287 \n",
      "Epoch: 194 | MAE Train Loss: 16.281860638869702 current Weight -> [0.54846068 0.8490719 ] ,current bias -> 0.30996221566228244 \n",
      "Epoch: 195 | MAE Train Loss: 16.236223769335226 current Weight -> [0.55127389 0.85343515] ,current bias -> 0.3114062141016972 \n",
      "Epoch: 196 | MAE Train Loss: 16.19065771993661 current Weight -> [0.5540871  0.85779849] ,current bias -> 0.3128488264389874 \n",
      "Epoch: 197 | MAE Train Loss: 16.145162479990105 current Weight -> [0.55690031 0.86216191] ,current bias -> 0.3142900541070844 \n",
      "Epoch: 198 | MAE Train Loss: 16.09973803883708 current Weight -> [0.55971353 0.86652542] ,current bias -> 0.3157298985374382 \n",
      "Epoch: 199 | MAE Train Loss: 16.05438438584399 current Weight -> [0.56252676 0.87088901] ,current bias -> 0.3171683611600191 \n",
      "Epoch: 200 | MAE Train Loss: 16.009101510402285 current Weight -> [0.56533999 0.87525269] ,current bias -> 0.3186054434033191 \n",
      "Epoch: 201 | MAE Train Loss: 15.9638894019284 current Weight -> [0.56815323 0.87961646] ,current bias -> 0.32004114669435335 \n",
      "Epoch: 202 | MAE Train Loss: 15.918748049863643 current Weight -> [0.57096647 0.88398031] ,current bias -> 0.32147547245866187 \n",
      "Epoch: 203 | MAE Train Loss: 15.873677443674206 current Weight -> [0.57377972 0.88834425] ,current bias -> 0.32290842212031096 \n",
      "Epoch: 204 | MAE Train Loss: 15.828677572851038 current Weight -> [0.57659298 0.89270827] ,current bias -> 0.3243399971018947 \n",
      "Epoch: 205 | MAE Train Loss: 15.783748426909838 current Weight -> [0.57940624 0.89707238] ,current bias -> 0.3257701988245366 \n",
      "Epoch: 206 | MAE Train Loss: 15.738889995390993 current Weight -> [0.5822195  0.90143657] ,current bias -> 0.3271990287078909 \n",
      "Epoch: 207 | MAE Train Loss: 15.694102267859499 current Weight -> [0.58503277 0.90580085] ,current bias -> 0.3286264881701444 \n",
      "Epoch: 208 | MAE Train Loss: 15.649385233904928 current Weight -> [0.58784605 0.91016521] ,current bias -> 0.33005257862801757 \n",
      "Epoch: 209 | MAE Train Loss: 15.60473888314136 current Weight -> [0.59065933 0.91452966] ,current bias -> 0.3314773014967665 \n",
      "Epoch: 210 | MAE Train Loss: 15.560163205207344 current Weight -> [0.59347262 0.91889419] ,current bias -> 0.33290065819018416 \n",
      "Epoch: 211 | MAE Train Loss: 15.515658189765814 current Weight -> [0.59628591 0.92325881] ,current bias -> 0.3343226501206019 \n",
      "Epoch: 212 | MAE Train Loss: 15.471223826504072 current Weight -> [0.59909921 0.92762351] ,current bias -> 0.3357432786988911 \n",
      "Epoch: 213 | MAE Train Loss: 15.426860105133692 current Weight -> [0.60191252 0.93198829] ,current bias -> 0.3371625453344645 \n",
      "Epoch: 214 | MAE Train Loss: 15.3825670153905 current Weight -> [0.60472582 0.93635316] ,current bias -> 0.3385804514352779 \n",
      "Epoch: 215 | MAE Train Loss: 15.338344547034513 current Weight -> [0.60753914 0.94071812] ,current bias -> 0.33999699840783154 \n",
      "Epoch: 216 | MAE Train Loss: 15.294192689849854 current Weight -> [0.61035246 0.94508316] ,current bias -> 0.3414121876571717 \n",
      "Epoch: 217 | MAE Train Loss: 15.25011143364473 current Weight -> [0.61316579 0.94944828] ,current bias -> 0.34282602058689204 \n",
      "Epoch: 218 | MAE Train Loss: 15.206100768251378 current Weight -> [0.61597912 0.95381349] ,current bias -> 0.3442384985991353 \n",
      "Epoch: 219 | MAE Train Loss: 15.162160683525988 current Weight -> [0.61879245 0.95817878] ,current bias -> 0.3456496230945946 \n",
      "Epoch: 220 | MAE Train Loss: 15.118291169348677 current Weight -> [0.6216058  0.96254416] ,current bias -> 0.34705939547251524 \n",
      "Epoch: 221 | MAE Train Loss: 15.0744922156234 current Weight -> [0.62441914 0.96690962] ,current bias -> 0.34846781713069575 \n",
      "Epoch: 222 | MAE Train Loss: 15.030763812277925 current Weight -> [0.6272325  0.97127516] ,current bias -> 0.34987488946548984 \n",
      "Epoch: 223 | MAE Train Loss: 14.987105949263782 current Weight -> [0.63004586 0.97564079] ,current bias -> 0.3512806138718076 \n",
      "Epoch: 224 | MAE Train Loss: 14.943518616556183 current Weight -> [0.63285922 0.9800065 ] ,current bias -> 0.352684991743117 \n",
      "Epoch: 225 | MAE Train Loss: 14.90000180415398 current Weight -> [0.63567259 0.98437229] ,current bias -> 0.3540880244714457 \n",
      "Epoch: 226 | MAE Train Loss: 14.856555502079631 current Weight -> [0.63848596 0.98873817] ,current bias -> 0.3554897134473821 \n",
      "Epoch: 227 | MAE Train Loss: 14.813179700379125 current Weight -> [0.64129934 0.99310413] ,current bias -> 0.35689006006007706 \n",
      "Epoch: 228 | MAE Train Loss: 14.769874389121926 current Weight -> [0.64411273 0.99747017] ,current bias -> 0.35828906569724545 \n",
      "Epoch: 229 | MAE Train Loss: 14.726639558400937 current Weight -> [0.64692612 1.0018363 ] ,current bias -> 0.3596867317451674 \n",
      "Epoch: 230 | MAE Train Loss: 14.68347519833244 current Weight -> [0.64973952 1.00620251] ,current bias -> 0.3610830595886901 \n",
      "Epoch: 231 | MAE Train Loss: 14.640381299056038 current Weight -> [0.65255292 1.0105688 ] ,current bias -> 0.36247805061122895 \n",
      "Epoch: 232 | MAE Train Loss: 14.597357850734602 current Weight -> [0.65536632 1.01493518] ,current bias -> 0.3638717061947693 \n",
      "Epoch: 233 | MAE Train Loss: 14.554404843554234 current Weight -> [0.65817974 1.01930164] ,current bias -> 0.3652640277198677 \n",
      "Epoch: 234 | MAE Train Loss: 14.5115222677242 current Weight -> [0.66099316 1.02366818] ,current bias -> 0.3666550165656537 \n",
      "Epoch: 235 | MAE Train Loss: 14.468710113476869 current Weight -> [0.66380658 1.0280348 ] ,current bias -> 0.36804467410983094 \n",
      "Epoch: 236 | MAE Train Loss: 14.425968371067684 current Weight -> [0.66662001 1.03240151] ,current bias -> 0.3694330017286789 \n",
      "Epoch: 237 | MAE Train Loss: 14.38329703077511 current Weight -> [0.66943344 1.0367683 ] ,current bias -> 0.37082000079705424 \n",
      "Epoch: 238 | MAE Train Loss: 14.340696082900537 current Weight -> [0.67224688 1.04113517] ,current bias -> 0.37220567268839233 \n",
      "Epoch: 239 | MAE Train Loss: 14.298165517768302 current Weight -> [0.67506032 1.04550213] ,current bias -> 0.37359001877470877 \n",
      "Epoch: 240 | MAE Train Loss: 14.255705325725565 current Weight -> [0.67787377 1.04986916] ,current bias -> 0.3749730404266007 \n",
      "Epoch: 241 | MAE Train Loss: 14.213315497142307 current Weight -> [0.68068723 1.05423628] ,current bias -> 0.37635473901324834 \n",
      "Epoch: 242 | MAE Train Loss: 14.170996022411265 current Weight -> [0.68350069 1.05860348] ,current bias -> 0.3777351159024166 \n",
      "Epoch: 243 | MAE Train Loss: 14.128746891947854 current Weight -> [0.68631416 1.06297076] ,current bias -> 0.3791141724604563 \n",
      "Epoch: 244 | MAE Train Loss: 14.086568096190165 current Weight -> [0.68912763 1.06733813] ,current bias -> 0.3804919100523058 \n",
      "Epoch: 245 | MAE Train Loss: 14.044459625598863 current Weight -> [0.6919411  1.07170557] ,current bias -> 0.3818683300414924 \n",
      "Epoch: 246 | MAE Train Loss: 14.002421470657179 current Weight -> [0.69475458 1.0760731 ] ,current bias -> 0.38324343379013376 \n",
      "Epoch: 247 | MAE Train Loss: 13.96045362187083 current Weight -> [0.69756807 1.08044071] ,current bias -> 0.38461722265893944 \n",
      "Epoch: 248 | MAE Train Loss: 13.918556069767979 current Weight -> [0.70038156 1.0848084 ] ,current bias -> 0.38598969800721233 \n",
      "Epoch: 249 | MAE Train Loss: 13.876728804899196 current Weight -> [0.70319506 1.08917618] ,current bias -> 0.3873608611928501 \n",
      "Epoch: 250 | MAE Train Loss: 13.834971817837381 current Weight -> [0.70600856 1.09354403] ,current bias -> 0.3887307135723467 \n",
      "Epoch: 251 | MAE Train Loss: 13.79328509917772 current Weight -> [0.70882207 1.09791196] ,current bias -> 0.3900992565007935 \n",
      "Epoch: 252 | MAE Train Loss: 13.75166863953766 current Weight -> [0.71163559 1.10227998] ,current bias -> 0.3914664913318814 \n",
      "Epoch: 253 | MAE Train Loss: 13.71012242955684 current Weight -> [0.7144491  1.10664808] ,current bias -> 0.3928324194179015 \n",
      "Epoch: 254 | MAE Train Loss: 13.66864645989703 current Weight -> [0.71726263 1.11101626] ,current bias -> 0.3941970421097471 \n",
      "Epoch: 255 | MAE Train Loss: 13.627240721242096 current Weight -> [0.72007616 1.11538452] ,current bias -> 0.395560360756915 \n",
      "Epoch: 256 | MAE Train Loss: 13.585905204297944 current Weight -> [0.72288969 1.11975286] ,current bias -> 0.3969223767075068 \n",
      "Epoch: 257 | MAE Train Loss: 13.544639899792491 current Weight -> [0.72570323 1.12412128] ,current bias -> 0.3982830913082306 \n",
      "Epoch: 258 | MAE Train Loss: 13.503444798475568 current Weight -> [0.72851678 1.12848978] ,current bias -> 0.39964250590440215 \n",
      "Epoch: 259 | MAE Train Loss: 13.462319891118916 current Weight -> [0.73133033 1.13285836] ,current bias -> 0.4010006218399465 \n",
      "Epoch: 260 | MAE Train Loss: 13.421265168516122 current Weight -> [0.73414388 1.13722703] ,current bias -> 0.40235744045739935 \n",
      "Epoch: 261 | MAE Train Loss: 13.380280621482562 current Weight -> [0.73695744 1.14159577] ,current bias -> 0.4037129630979085 \n",
      "Epoch: 262 | MAE Train Loss: 13.33936624085536 current Weight -> [0.73977101 1.14596459] ,current bias -> 0.4050671911012353 \n",
      "Epoch: 263 | MAE Train Loss: 13.298522017493326 current Weight -> [0.74258458 1.1503335 ] ,current bias -> 0.4064201258057561 \n",
      "Epoch: 264 | MAE Train Loss: 13.257747942276929 current Weight -> [0.74539815 1.15470248] ,current bias -> 0.40777176854846364 \n",
      "Epoch: 265 | MAE Train Loss: 13.217044006108232 current Weight -> [0.74821174 1.15907155] ,current bias -> 0.4091221206649685 \n",
      "Epoch: 266 | MAE Train Loss: 13.17641019991084 current Weight -> [0.75102532 1.16344069] ,current bias -> 0.4104711834895005 \n",
      "Epoch: 267 | MAE Train Loss: 13.135846514629863 current Weight -> [0.75383891 1.16780991] ,current bias -> 0.4118189583549102 \n",
      "Epoch: 268 | MAE Train Loss: 13.09535294123187 current Weight -> [0.75665251 1.17217922] ,current bias -> 0.4131654465926703 \n",
      "Epoch: 269 | MAE Train Loss: 13.054929470704819 current Weight -> [0.75946611 1.1765486 ] ,current bias -> 0.4145106495328769 \n",
      "Epoch: 270 | MAE Train Loss: 13.014576094058032 current Weight -> [0.76227972 1.18091807] ,current bias -> 0.41585456850425123 \n",
      "Epoch: 271 | MAE Train Loss: 12.974292802322132 current Weight -> [0.76509333 1.18528761] ,current bias -> 0.4171972048341408 \n",
      "Epoch: 272 | MAE Train Loss: 12.934079586549 current Weight -> [0.76790695 1.18965723] ,current bias -> 0.418538559848521 \n",
      "Epoch: 273 | MAE Train Loss: 12.893936437811728 current Weight -> [0.77072057 1.19402694] ,current bias -> 0.4198786348719964 \n",
      "Epoch: 274 | MAE Train Loss: 12.853863347204577 current Weight -> [0.7735342  1.19839672] ,current bias -> 0.42121743122780225 \n",
      "Epoch: 275 | MAE Train Loss: 12.8138603058429 current Weight -> [0.77634783 1.20276658] ,current bias -> 0.4225549502378058 \n",
      "Epoch: 276 | MAE Train Loss: 12.77392730486314 current Weight -> [0.77916147 1.20713652] ,current bias -> 0.4238911932225079 \n",
      "Epoch: 277 | MAE Train Loss: 12.734064335422742 current Weight -> [0.78197511 1.21150654] ,current bias -> 0.42522616150104414 \n",
      "Epoch: 278 | MAE Train Loss: 12.694271388700129 current Weight -> [0.78478876 1.21587664] ,current bias -> 0.42655985639118643 \n",
      "Epoch: 279 | MAE Train Loss: 12.654548455894647 current Weight -> [0.78760242 1.22024681] ,current bias -> 0.4278922792093445 \n",
      "Epoch: 280 | MAE Train Loss: 12.614895528226512 current Weight -> [0.79041607 1.22461707] ,current bias -> 0.42922343127056706 \n",
      "Epoch: 281 | MAE Train Loss: 12.575312596936776 current Weight -> [0.79322974 1.22898741] ,current bias -> 0.43055331388854345 \n",
      "Epoch: 282 | MAE Train Loss: 12.535799653287265 current Weight -> [0.79604341 1.23335782] ,current bias -> 0.4318819283756049 \n",
      "Epoch: 283 | MAE Train Loss: 12.496356688560539 current Weight -> [0.79885708 1.23772831] ,current bias -> 0.4332092760427259 \n",
      "Epoch: 284 | MAE Train Loss: 12.456983694059844 current Weight -> [0.80167076 1.24209888] ,current bias -> 0.4345353581995258 \n",
      "Epoch: 285 | MAE Train Loss: 12.417680661109078 current Weight -> [0.80448444 1.24646953] ,current bias -> 0.43586017615427003 \n",
      "Epoch: 286 | MAE Train Loss: 12.378447581052722 current Weight -> [0.80729813 1.25084026] ,current bias -> 0.4371837312138715 \n",
      "Epoch: 287 | MAE Train Loss: 12.339284445255796 current Weight -> [0.81011183 1.25521107] ,current bias -> 0.4385060246838921 \n",
      "Epoch: 288 | MAE Train Loss: 12.300191245103838 current Weight -> [0.81292553 1.25958195] ,current bias -> 0.4398270578685441 \n",
      "Epoch: 289 | MAE Train Loss: 12.261167972002829 current Weight -> [0.81573923 1.26395292] ,current bias -> 0.4411468320706914 \n",
      "Epoch: 290 | MAE Train Loss: 12.222214617379159 current Weight -> [0.81855294 1.26832396] ,current bias -> 0.44246534859185105 \n",
      "Epoch: 291 | MAE Train Loss: 12.183331172679571 current Weight -> [0.82136665 1.27269507] ,current bias -> 0.4437826087321947 \n",
      "Epoch: 292 | MAE Train Loss: 12.144517629371135 current Weight -> [0.82418037 1.27706627] ,current bias -> 0.4450986137905498 \n",
      "Epoch: 293 | MAE Train Loss: 12.10577397894118 current Weight -> [0.8269941  1.28143755] ,current bias -> 0.4464133650644011 \n",
      "Epoch: 294 | MAE Train Loss: 12.067100212897266 current Weight -> [0.82980783 1.2858089 ] ,current bias -> 0.4477268638498921 \n",
      "Epoch: 295 | MAE Train Loss: 12.02849632276713 current Weight -> [0.83262156 1.29018033] ,current bias -> 0.4490391114418262 \n",
      "Epoch: 296 | MAE Train Loss: 11.989962300098624 current Weight -> [0.8354353  1.29455184] ,current bias -> 0.45035010913366846 \n",
      "Epoch: 297 | MAE Train Loss: 11.951498136459701 current Weight -> [0.83824905 1.29892342] ,current bias -> 0.4516598582175467 \n",
      "Epoch: 298 | MAE Train Loss: 11.913103823438355 current Weight -> [0.8410628  1.30329508] ,current bias -> 0.4529683599842529 \n",
      "Epoch: 299 | MAE Train Loss: 11.874779352642555 current Weight -> [0.84387655 1.30766682] ,current bias -> 0.45427561572324465 \n",
      "Epoch: 300 | MAE Train Loss: 11.836524715700248 current Weight -> [0.84669031 1.31203864] ,current bias -> 0.4555816267226466 \n",
      "Epoch: 301 | MAE Train Loss: 11.798339904259256 current Weight -> [0.84950408 1.31641054] ,current bias -> 0.45688639426925176 \n",
      "Epoch: 302 | MAE Train Loss: 11.760224909987274 current Weight -> [0.85231785 1.32078251] ,current bias -> 0.45818991964852274 \n",
      "Epoch: 303 | MAE Train Loss: 11.722179724571806 current Weight -> [0.85513162 1.32515456] ,current bias -> 0.45949220414459335 \n",
      "Epoch: 304 | MAE Train Loss: 11.684204339720138 current Weight -> [0.8579454  1.32952668] ,current bias -> 0.46079324904026986 \n",
      "Epoch: 305 | MAE Train Loss: 11.646298747159253 current Weight -> [0.86075918 1.33389888] ,current bias -> 0.4620930556170324 \n",
      "Epoch: 306 | MAE Train Loss: 11.608462938635835 current Weight -> [0.86357297 1.33827116] ,current bias -> 0.46339162515503635 \n",
      "Epoch: 307 | MAE Train Loss: 11.570696905916195 current Weight -> [0.86638677 1.34264352] ,current bias -> 0.4646889589331136 \n",
      "Epoch: 308 | MAE Train Loss: 11.533000640786229 current Weight -> [0.86920057 1.34701595] ,current bias -> 0.4659850582287742 \n",
      "Epoch: 309 | MAE Train Loss: 11.495374135051382 current Weight -> [0.87201437 1.35138846] ,current bias -> 0.46727992431820725 \n",
      "Epoch: 310 | MAE Train Loss: 11.457817380536598 current Weight -> [0.87482818 1.35576105] ,current bias -> 0.4685735584762828 \n",
      "Epoch: 311 | MAE Train Loss: 11.420330369086278 current Weight -> [0.877642   1.36013371] ,current bias -> 0.4698659619765528 \n",
      "Epoch: 312 | MAE Train Loss: 11.38291309256423 current Weight -> [0.88045582 1.36450645] ,current bias -> 0.47115713609125265 \n",
      "Epoch: 313 | MAE Train Loss: 11.345565542853642 current Weight -> [0.88326964 1.36887926] ,current bias -> 0.4724470820913026 \n",
      "Epoch: 314 | MAE Train Loss: 11.308287711857021 current Weight -> [0.88608347 1.37325215] ,current bias -> 0.47373580124630893 \n",
      "Epoch: 315 | MAE Train Loss: 11.271079591496136 current Weight -> [0.8888973  1.37762512] ,current bias -> 0.4750232948245655 \n",
      "Epoch: 316 | MAE Train Loss: 11.23394117371202 current Weight -> [0.89171114 1.38199816] ,current bias -> 0.4763095640930551 \n",
      "Epoch: 317 | MAE Train Loss: 11.196872450464888 current Weight -> [0.89452499 1.38637128] ,current bias -> 0.47759461031745054 \n",
      "Epoch: 318 | MAE Train Loss: 11.15987341373409 current Weight -> [0.89733884 1.39074447] ,current bias -> 0.47887843476211633 \n",
      "Epoch: 319 | MAE Train Loss: 11.122944055518106 current Weight -> [0.90015269 1.39511774] ,current bias -> 0.4801610386901099 \n",
      "Epoch: 320 | MAE Train Loss: 11.08608436783445 current Weight -> [0.90296655 1.39949109] ,current bias -> 0.4814424233631829 \n",
      "Epoch: 321 | MAE Train Loss: 11.04929434271968 current Weight -> [0.90578041 1.40386451] ,current bias -> 0.4827225900417826 \n",
      "Epoch: 322 | MAE Train Loss: 11.012573972229315 current Weight -> [0.90859428 1.40823801] ,current bias -> 0.48400153998505324 \n",
      "Epoch: 323 | MAE Train Loss: 10.975923248437805 current Weight -> [0.91140815 1.41261158] ,current bias -> 0.48527927445083746 \n",
      "Epoch: 324 | MAE Train Loss: 10.939342163438505 current Weight -> [0.91422203 1.41698523] ,current bias -> 0.48655579469567745 \n",
      "Epoch: 325 | MAE Train Loss: 10.902830709343593 current Weight -> [0.91703592 1.42135895] ,current bias -> 0.4878311019748165 \n",
      "Epoch: 326 | MAE Train Loss: 10.866388878284067 current Weight -> [0.9198498  1.42573275] ,current bias -> 0.4891051975422001 \n",
      "Epoch: 327 | MAE Train Loss: 10.830016662409676 current Weight -> [0.9226637  1.43010662] ,current bias -> 0.49037808265047766 \n",
      "Epoch: 328 | MAE Train Loss: 10.793714053888898 current Weight -> [0.92547759 1.43448057] ,current bias -> 0.4916497585510034 \n",
      "Epoch: 329 | MAE Train Loss: 10.757481044908868 current Weight -> [0.9282915 1.4388546] ,current bias -> 0.49292022649383804 \n",
      "Epoch: 330 | MAE Train Loss: 10.721317627675377 current Weight -> [0.9311054 1.4432287] ,current bias -> 0.49418948772774995 \n",
      "Epoch: 331 | MAE Train Loss: 10.68522379441279 current Weight -> [0.93391932 1.44760287] ,current bias -> 0.49545754350021654 \n",
      "Epoch: 332 | MAE Train Loss: 10.649199537364023 current Weight -> [0.93673323 1.45197712] ,current bias -> 0.4967243950574256 \n",
      "Epoch: 333 | MAE Train Loss: 10.613244848790501 current Weight -> [0.93954716 1.45635144] ,current bias -> 0.4979900436442767 \n",
      "Epoch: 334 | MAE Train Loss: 10.577359720972108 current Weight -> [0.94236108 1.46072584] ,current bias -> 0.49925449050438236 \n",
      "Epoch: 335 | MAE Train Loss: 10.541544146207151 current Weight -> [0.94517502 1.46510031] ,current bias -> 0.5005177368800695 \n",
      "Epoch: 336 | MAE Train Loss: 10.505798116812326 current Weight -> [0.94798895 1.46947486] ,current bias -> 0.5017797840123808 \n",
      "Epoch: 337 | MAE Train Loss: 10.47012162512265 current Weight -> [0.95080289 1.47384948] ,current bias -> 0.5030406331410758 \n",
      "Epoch: 338 | MAE Train Loss: 10.43451466349145 current Weight -> [0.95361684 1.47822417] ,current bias -> 0.5043002855046327 \n",
      "Epoch: 339 | MAE Train Loss: 10.398977224290304 current Weight -> [0.95643079 1.48259894] ,current bias -> 0.505558742340249 \n",
      "Epoch: 340 | MAE Train Loss: 10.363509299909001 current Weight -> [0.95924475 1.48697379] ,current bias -> 0.5068160048838435 \n",
      "Epoch: 341 | MAE Train Loss: 10.328110882755503 current Weight -> [0.96205871 1.4913487 ] ,current bias -> 0.5080720743700572 \n",
      "Epoch: 342 | MAE Train Loss: 10.292781965255896 current Weight -> [0.96487267 1.49572369] ,current bias -> 0.5093269520322548 \n",
      "Epoch: 343 | MAE Train Loss: 10.257522539854367 current Weight -> [0.96768664 1.50009876] ,current bias -> 0.5105806391025259 \n",
      "Epoch: 344 | MAE Train Loss: 10.22233259901314 current Weight -> [0.97050062 1.5044739 ] ,current bias -> 0.5118331368116865 \n",
      "Epoch: 345 | MAE Train Loss: 10.18721213521245 current Weight -> [0.9733146  1.50884911] ,current bias -> 0.5130844463892801 \n",
      "Epoch: 346 | MAE Train Loss: 10.152161140950497 current Weight -> [0.97612858 1.5132244 ] ,current bias -> 0.514334569063579 \n",
      "Epoch: 347 | MAE Train Loss: 10.117179608743406 current Weight -> [0.97894257 1.51759976] ,current bias -> 0.515583506061586 \n",
      "Epoch: 348 | MAE Train Loss: 10.082267531125183 current Weight -> [0.98175657 1.52197519] ,current bias -> 0.5168312586090352 \n",
      "Epoch: 349 | MAE Train Loss: 10.047424900647684 current Weight -> [0.98457056 1.5263507 ] ,current bias -> 0.5180778279303938 \n",
      "Epoch: 350 | MAE Train Loss: 10.012651709880554 current Weight -> [0.98738457 1.53072628] ,current bias -> 0.5193232152488629 \n",
      "Epoch: 351 | MAE Train Loss: 9.977947951411215 current Weight -> [0.99019858 1.53510194] ,current bias -> 0.5205674217863794 \n",
      "Epoch: 352 | MAE Train Loss: 9.943313617844801 current Weight -> [0.99301259 1.53947766] ,current bias -> 0.5218104487636166 \n",
      "Epoch: 353 | MAE Train Loss: 9.908748701804127 current Weight -> [0.99582661 1.54385346] ,current bias -> 0.5230522973999862 \n",
      "Epoch: 354 | MAE Train Loss: 9.874253195929654 current Weight -> [0.99864063 1.54822934] ,current bias -> 0.524292968913639 \n",
      "Epoch: 355 | MAE Train Loss: 9.839827092879437 current Weight -> [1.00145466 1.55260528] ,current bias -> 0.5255324645214667 \n",
      "Epoch: 356 | MAE Train Loss: 9.805470385329098 current Weight -> [1.00426869 1.5569813 ] ,current bias -> 0.526770785439103 \n",
      "Epoch: 357 | MAE Train Loss: 9.771183065971776 current Weight -> [1.00708273 1.5613574 ] ,current bias -> 0.528007932880925 \n",
      "Epoch: 358 | MAE Train Loss: 9.736965127518083 current Weight -> [1.00989677 1.56573356] ,current bias -> 0.529243908060054 \n",
      "Epoch: 359 | MAE Train Loss: 9.70281656269608 current Weight -> [1.01271081 1.5701098 ] ,current bias -> 0.5304787121883574 \n",
      "Epoch: 360 | MAE Train Loss: 9.668737364251236 current Weight -> [1.01552486 1.57448611] ,current bias -> 0.53171234647645 \n",
      "Epoch: 361 | MAE Train Loss: 9.634727524946358 current Weight -> [1.01833892 1.57886249] ,current bias -> 0.5329448121336949 \n",
      "Epoch: 362 | MAE Train Loss: 9.600787037561593 current Weight -> [1.02115298 1.58323895] ,current bias -> 0.534176110368205 \n",
      "Epoch: 363 | MAE Train Loss: 9.566915894894365 current Weight -> [1.02396705 1.58761548] ,current bias -> 0.5354062423868441 \n",
      "Epoch: 364 | MAE Train Loss: 9.533114089759339 current Weight -> [1.02678111 1.59199208] ,current bias -> 0.5366352093952287 \n",
      "Epoch: 365 | MAE Train Loss: 9.499381614988387 current Weight -> [1.02959519 1.59636875] ,current bias -> 0.5378630125977286 \n",
      "Epoch: 366 | MAE Train Loss: 9.46571846343054 current Weight -> [1.03240927 1.60074549] ,current bias -> 0.5390896531974689 \n",
      "Epoch: 367 | MAE Train Loss: 9.432124627951952 current Weight -> [1.03522335 1.60512231] ,current bias -> 0.5403151323963307 \n",
      "Epoch: 368 | MAE Train Loss: 9.398600101435868 current Weight -> [1.03803744 1.6094992 ] ,current bias -> 0.5415394513949525 \n",
      "Epoch: 369 | MAE Train Loss: 9.365144876782578 current Weight -> [1.04085153 1.61387616] ,current bias -> 0.542762611392732 \n",
      "Epoch: 370 | MAE Train Loss: 9.331758946909378 current Weight -> [1.04366563 1.61825319] ,current bias -> 0.5439846135878265 \n",
      "Epoch: 371 | MAE Train Loss: 9.298442304750534 current Weight -> [1.04647973 1.6226303 ] ,current bias -> 0.545205459177155 \n",
      "Epoch: 372 | MAE Train Loss: 9.265194943257237 current Weight -> [1.04929384 1.62700747] ,current bias -> 0.5464251493563991 \n",
      "Epoch: 373 | MAE Train Loss: 9.232016855397582 current Weight -> [1.05210795 1.63138472] ,current bias -> 0.5476436853200042 \n",
      "Epoch: 374 | MAE Train Loss: 9.198908034156503 current Weight -> [1.05492207 1.63576204] ,current bias -> 0.548861068261181 \n",
      "Epoch: 375 | MAE Train Loss: 9.165868472535754 current Weight -> [1.05773619 1.64013943] ,current bias -> 0.5500772993719065 \n",
      "Epoch: 376 | MAE Train Loss: 9.132898163553868 current Weight -> [1.06055031 1.64451689] ,current bias -> 0.5512923798429258 \n",
      "Epoch: 377 | MAE Train Loss: 9.09999710024611 current Weight -> [1.06336444 1.64889442] ,current bias -> 0.5525063108637527 \n",
      "Epoch: 378 | MAE Train Loss: 9.06716527566445 current Weight -> [1.06617858 1.65327203] ,current bias -> 0.5537190936226715 \n",
      "Epoch: 379 | MAE Train Loss: 9.034402682877513 current Weight -> [1.06899272 1.65764971] ,current bias -> 0.5549307293067379 \n",
      "Epoch: 380 | MAE Train Loss: 9.001709314970553 current Weight -> [1.07180686 1.66202745] ,current bias -> 0.5561412191017806 \n",
      "Epoch: 381 | MAE Train Loss: 8.969085165045403 current Weight -> [1.07462101 1.66640527] ,current bias -> 0.5573505641924023 \n",
      "Epoch: 382 | MAE Train Loss: 8.936530226220448 current Weight -> [1.07743516 1.67078316] ,current bias -> 0.558558765761981 \n",
      "Epoch: 383 | MAE Train Loss: 8.90404449163058 current Weight -> [1.08024932 1.67516112] ,current bias -> 0.5597658249926717 \n",
      "Epoch: 384 | MAE Train Loss: 8.87162795442717 current Weight -> [1.08306349 1.67953915] ,current bias -> 0.5609717430654069 \n",
      "Epoch: 385 | MAE Train Loss: 8.839280607778012 current Weight -> [1.08587765 1.68391725] ,current bias -> 0.5621765211598984 \n",
      "Epoch: 386 | MAE Train Loss: 8.807002444867301 current Weight -> [1.08869182 1.68829543] ,current bias -> 0.5633801604546386 \n",
      "Epoch: 387 | MAE Train Loss: 8.774793458895594 current Weight -> [1.091506   1.69267367] ,current bias -> 0.5645826621269013 \n",
      "Epoch: 388 | MAE Train Loss: 8.742653643079764 current Weight -> [1.09432018 1.69705198] ,current bias -> 0.5657840273527436 \n",
      "Epoch: 389 | MAE Train Loss: 8.71058299065298 current Weight -> [1.09713437 1.70143037] ,current bias -> 0.5669842573070064 \n",
      "Epoch: 390 | MAE Train Loss: 8.678581494864641 current Weight -> [1.09994856 1.70580882] ,current bias -> 0.5681833531633164 \n",
      "Epoch: 391 | MAE Train Loss: 8.646649148980368 current Weight -> [1.10276275 1.71018735] ,current bias -> 0.5693813160940868 \n",
      "Epoch: 392 | MAE Train Loss: 8.61478594628195 current Weight -> [1.10557695 1.71456594] ,current bias -> 0.570578147270519 \n",
      "Epoch: 393 | MAE Train Loss: 8.582991880067317 current Weight -> [1.10839115 1.71894461] ,current bias -> 0.5717738478626034 \n",
      "Epoch: 394 | MAE Train Loss: 8.55126694365049 current Weight -> [1.11120536 1.72332334] ,current bias -> 0.5729684190391211 \n",
      "Epoch: 395 | MAE Train Loss: 8.51961113036156 current Weight -> [1.11401957 1.72770215] ,current bias -> 0.5741618619676448 \n",
      "Epoch: 396 | MAE Train Loss: 8.488024433546636 current Weight -> [1.11683379 1.73208102] ,current bias -> 0.5753541778145402 \n",
      "Epoch: 397 | MAE Train Loss: 8.456506846567823 current Weight -> [1.11964801 1.73645997] ,current bias -> 0.5765453677449672 \n",
      "Epoch: 398 | MAE Train Loss: 8.425058362803178 current Weight -> [1.12246224 1.74083898] ,current bias -> 0.5777354329228813 \n",
      "Epoch: 399 | MAE Train Loss: 8.393678975646663 current Weight -> [1.12527647 1.74521807] ,current bias -> 0.5789243745110345 \n",
      "Epoch: 400 | MAE Train Loss: 8.362368678508131 current Weight -> [1.1280907  1.74959722] ,current bias -> 0.580112193670977 \n",
      "Epoch: 401 | MAE Train Loss: 8.331127464813276 current Weight -> [1.13090494 1.75397645] ,current bias -> 0.581298891563058 \n",
      "Epoch: 402 | MAE Train Loss: 8.299955328003593 current Weight -> [1.13371919 1.75835574] ,current bias -> 0.5824844693464274 \n",
      "Epoch: 403 | MAE Train Loss: 8.26885226153635 current Weight -> [1.13653344 1.7627351 ] ,current bias -> 0.5836689281790365 \n",
      "Epoch: 404 | MAE Train Loss: 8.237818258884552 current Weight -> [1.13934769 1.76711454] ,current bias -> 0.5848522692176398 \n",
      "Epoch: 405 | MAE Train Loss: 8.206853313536898 current Weight -> [1.14216195 1.77149404] ,current bias -> 0.5860344936177957 \n",
      "Epoch: 406 | MAE Train Loss: 8.175957418997754 current Weight -> [1.14497621 1.77587361] ,current bias -> 0.5872156025338682 \n",
      "Epoch: 407 | MAE Train Loss: 8.14513056878711 current Weight -> [1.14779048 1.78025325] ,current bias -> 0.5883955971190278 \n",
      "Epoch: 408 | MAE Train Loss: 8.11437275644055 current Weight -> [1.15060475 1.78463296] ,current bias -> 0.589574478525253 \n",
      "Epoch: 409 | MAE Train Loss: 8.083683975509205 current Weight -> [1.15341902 1.78901274] ,current bias -> 0.5907522479033314 \n",
      "Epoch: 410 | MAE Train Loss: 8.05306421955973 current Weight -> [1.1562333  1.79339259] ,current bias -> 0.5919289064028607 \n",
      "Epoch: 411 | MAE Train Loss: 8.022513482174269 current Weight -> [1.15904759 1.7977725 ] ,current bias -> 0.5931044551722504 \n",
      "Epoch: 412 | MAE Train Loss: 7.992031756950405 current Weight -> [1.16186188 1.80215249] ,current bias -> 0.5942788953587229 \n",
      "Epoch: 413 | MAE Train Loss: 7.961619037501141 current Weight -> [1.16467617 1.80653254] ,current bias -> 0.5954522281083143 \n",
      "Epoch: 414 | MAE Train Loss: 7.931275317454857 current Weight -> [1.16749047 1.81091266] ,current bias -> 0.5966244545658761 \n",
      "Epoch: 415 | MAE Train Loss: 7.901000590455269 current Weight -> [1.17030477 1.81529286] ,current bias -> 0.5977955758750763 \n",
      "Epoch: 416 | MAE Train Loss: 7.870794850161408 current Weight -> [1.17311908 1.81967312] ,current bias -> 0.5989655931784005 \n",
      "Epoch: 417 | MAE Train Loss: 7.8406580902475715 current Weight -> [1.17593339 1.82405344] ,current bias -> 0.6001345076171533 \n",
      "Epoch: 418 | MAE Train Loss: 7.810590304403302 current Weight -> [1.17874771 1.82843384] ,current bias -> 0.6013023203314594 \n",
      "Epoch: 419 | MAE Train Loss: 7.780591486333335 current Weight -> [1.18156203 1.83281431] ,current bias -> 0.6024690324602648 \n",
      "Epoch: 420 | MAE Train Loss: 7.750661629757577 current Weight -> [1.18437635 1.83719484] ,current bias -> 0.6036346451413384 \n",
      "Epoch: 421 | MAE Train Loss: 7.720800728411067 current Weight -> [1.18719068 1.84157544] ,current bias -> 0.6047991595112723 \n",
      "Epoch: 422 | MAE Train Loss: 7.691008776043942 current Weight -> [1.19000501 1.84595611] ,current bias -> 0.605962576705484 \n",
      "Epoch: 423 | MAE Train Loss: 7.6612857664214085 current Weight -> [1.19281935 1.85033685] ,current bias -> 0.6071248978582173 \n",
      "Epoch: 424 | MAE Train Loss: 7.631631693323689 current Weight -> [1.19563369 1.85471765] ,current bias -> 0.6082861241025433 \n",
      "Epoch: 425 | MAE Train Loss: 7.602046550546009 current Weight -> [1.19844804 1.85909853] ,current bias -> 0.6094462565703617 \n",
      "Epoch: 426 | MAE Train Loss: 7.572530331898555 current Weight -> [1.20126239 1.86347947] ,current bias -> 0.610605296392402 \n",
      "Epoch: 427 | MAE Train Loss: 7.543083031206432 current Weight -> [1.20407675 1.86786048] ,current bias -> 0.6117632446982251 \n",
      "Epoch: 428 | MAE Train Loss: 7.5137046423096425 current Weight -> [1.20689111 1.87224155] ,current bias -> 0.6129201026162238 \n",
      "Epoch: 429 | MAE Train Loss: 7.4843951590630455 current Weight -> [1.20970547 1.8766227 ] ,current bias -> 0.6140758712736244 \n",
      "Epoch: 430 | MAE Train Loss: 7.455154575336317 current Weight -> [1.21251984 1.88100391] ,current bias -> 0.6152305517964882 \n",
      "Epoch: 431 | MAE Train Loss: 7.425982885013932 current Weight -> [1.21533421 1.88538519] ,current bias -> 0.6163841453097123 \n",
      "Epoch: 432 | MAE Train Loss: 7.3968800819951115 current Weight -> [1.21814859 1.88976654] ,current bias -> 0.6175366529370306 \n",
      "Epoch: 433 | MAE Train Loss: 7.367846160193803 current Weight -> [1.22096297 1.89414795] ,current bias -> 0.6186880758010157 \n",
      "Epoch: 434 | MAE Train Loss: 7.338881113538641 current Weight -> [1.22377736 1.89852943] ,current bias -> 0.6198384150230795 \n",
      "Epoch: 435 | MAE Train Loss: 7.3099849359729125 current Weight -> [1.22659175 1.90291098] ,current bias -> 0.6209876717234745 \n",
      "Epoch: 436 | MAE Train Loss: 7.281157621454522 current Weight -> [1.22940614 1.9072926 ] ,current bias -> 0.6221358470212953 \n",
      "Epoch: 437 | MAE Train Loss: 7.2523991639559675 current Weight -> [1.23222054 1.91167428] ,current bias -> 0.6232829420344796 \n",
      "Epoch: 438 | MAE Train Loss: 7.223709557464292 current Weight -> [1.23503494 1.91605603] ,current bias -> 0.6244289578798092 \n",
      "Epoch: 439 | MAE Train Loss: 7.195088795981065 current Weight -> [1.23784935 1.92043785] ,current bias -> 0.6255738956729116 \n",
      "Epoch: 440 | MAE Train Loss: 7.166536873522339 current Weight -> [1.24066376 1.92481973] ,current bias -> 0.6267177565282608 \n",
      "Epoch: 441 | MAE Train Loss: 7.1380537841186165 current Weight -> [1.24347818 1.92920168] ,current bias -> 0.6278605415591787 \n",
      "Epoch: 442 | MAE Train Loss: 7.1096395218148265 current Weight -> [1.2462926 1.9335837] ,current bias -> 0.6290022518778364 \n",
      "Epoch: 443 | MAE Train Loss: 7.081294080670282 current Weight -> [1.24910703 1.93796578] ,current bias -> 0.6301428885952552 \n",
      "Epoch: 444 | MAE Train Loss: 7.053017454758645 current Weight -> [1.25192146 1.94234793] ,current bias -> 0.6312824528213079 \n",
      "Epoch: 445 | MAE Train Loss: 7.024809638167905 current Weight -> [1.25473589 1.94673015] ,current bias -> 0.6324209456647196 \n",
      "Epoch: 446 | MAE Train Loss: 6.996670625000335 current Weight -> [1.25755033 1.95111243] ,current bias -> 0.6335583682330697 \n",
      "Epoch: 447 | MAE Train Loss: 6.968600409372463 current Weight -> [1.26036477 1.95549478] ,current bias -> 0.6346947216327922 \n",
      "Epoch: 448 | MAE Train Loss: 6.9405989854150425 current Weight -> [1.26317922 1.9598772 ] ,current bias -> 0.6358300069691775 \n",
      "Epoch: 449 | MAE Train Loss: 6.912666347273013 current Weight -> [1.26599367 1.96425968] ,current bias -> 0.6369642253463733 \n",
      "Epoch: 450 | MAE Train Loss: 6.884802489105473 current Weight -> [1.26880812 1.96864223] ,current bias -> 0.638097377867386 \n",
      "Epoch: 451 | MAE Train Loss: 6.857007405085643 current Weight -> [1.27162258 1.97302484] ,current bias -> 0.6392294656340815 \n",
      "Epoch: 452 | MAE Train Loss: 6.829281089400838 current Weight -> [1.27443704 1.97740752] ,current bias -> 0.6403604897471866 \n",
      "Epoch: 453 | MAE Train Loss: 6.8016235362524275 current Weight -> [1.27725151 1.98179027] ,current bias -> 0.6414904513062905 \n",
      "Epoch: 454 | MAE Train Loss: 6.7740347398558205 current Weight -> [1.28006599 1.98617308] ,current bias -> 0.6426193514098453 \n",
      "Epoch: 455 | MAE Train Loss: 6.7465146944404015 current Weight -> [1.28288046 1.99055596] ,current bias -> 0.6437471911551677 \n",
      "Epoch: 456 | MAE Train Loss: 6.71906339424953 current Weight -> [1.28569494 1.9949389 ] ,current bias -> 0.64487397163844 \n",
      "Epoch: 457 | MAE Train Loss: 6.6916808335405 current Weight -> [1.28850943 1.99932191] ,current bias -> 0.6459996939547112 \n",
      "Epoch: 458 | MAE Train Loss: 6.6643670065844915 current Weight -> [1.29132392 2.00370499] ,current bias -> 0.6471243591978986 \n",
      "Epoch: 459 | MAE Train Loss: 6.637121907666563 current Weight -> [1.29413841 2.00808813] ,current bias -> 0.6482479684607881 \n",
      "Epoch: 460 | MAE Train Loss: 6.609945531085597 current Weight -> [1.29695291 2.01247134] ,current bias -> 0.6493705228350364 \n",
      "Epoch: 461 | MAE Train Loss: 6.582837871154287 current Weight -> [1.29976741 2.01685461] ,current bias -> 0.6504920234111712 \n",
      "Epoch: 462 | MAE Train Loss: 6.555798922199091 current Weight -> [1.30258192 2.02123794] ,current bias -> 0.6516124712785931 \n",
      "Epoch: 463 | MAE Train Loss: 6.528828678560216 current Weight -> [1.30539643 2.02562135] ,current bias -> 0.6527318675255765 \n",
      "Epoch: 464 | MAE Train Loss: 6.501927134591563 current Weight -> [1.30821094 2.03000481] ,current bias -> 0.6538502132392707 \n",
      "Epoch: 465 | MAE Train Loss: 6.4750942846607185 current Weight -> [1.31102546 2.03438835] ,current bias -> 0.654967509505701 \n",
      "Epoch: 466 | MAE Train Loss: 6.448330123148917 current Weight -> [1.31383998 2.03877194] ,current bias -> 0.6560837574097702 \n",
      "Epoch: 467 | MAE Train Loss: 6.421634644450995 current Weight -> [1.31665451 2.04315561] ,current bias -> 0.6571989580352594 \n",
      "Epoch: 468 | MAE Train Loss: 6.395007842975377 current Weight -> [1.31946904 2.04753934] ,current bias -> 0.6583131124648295 \n",
      "Epoch: 469 | MAE Train Loss: 6.36844971314404 current Weight -> [1.32228358 2.05192313] ,current bias -> 0.6594262217800219 \n",
      "Epoch: 470 | MAE Train Loss: 6.341960249392475 current Weight -> [1.32509812 2.05630699] ,current bias -> 0.6605382870612599 \n",
      "Epoch: 471 | MAE Train Loss: 6.3155394461696694 current Weight -> [1.32791266 2.06069091] ,current bias -> 0.6616493093878503 \n",
      "Epoch: 472 | MAE Train Loss: 6.289187297938059 current Weight -> [1.33072721 2.06507489] ,current bias -> 0.6627592898379835 \n",
      "Epoch: 473 | MAE Train Loss: 6.262903799173509 current Weight -> [1.33354176 2.06945895] ,current bias -> 0.6638682294887358 \n",
      "Epoch: 474 | MAE Train Loss: 6.236688944365284 current Weight -> [1.33635632 2.07384306] ,current bias -> 0.6649761294160699 \n",
      "Epoch: 475 | MAE Train Loss: 6.210542728016006 current Weight -> [1.33917088 2.07822724] ,current bias -> 0.666082990694836 \n",
      "Epoch: 476 | MAE Train Loss: 6.184465144641634 current Weight -> [1.34198545 2.08261149] ,current bias -> 0.6671888143987734 \n",
      "Epoch: 477 | MAE Train Loss: 6.158456188771432 current Weight -> [1.34480002 2.0869958 ] ,current bias -> 0.6682936016005112 \n",
      "Epoch: 478 | MAE Train Loss: 6.132515854947933 current Weight -> [1.34761459 2.09138017] ,current bias -> 0.6693973533715697 \n",
      "Epoch: 479 | MAE Train Loss: 6.1066441377269145 current Weight -> [1.35042917 2.09576461] ,current bias -> 0.6705000707823615 \n",
      "Epoch: 480 | MAE Train Loss: 6.080841031677361 current Weight -> [1.35324375 2.10014911] ,current bias -> 0.6716017549021925 \n",
      "Epoch: 481 | MAE Train Loss: 6.055106531381443 current Weight -> [1.35605834 2.10453368] ,current bias -> 0.6727024067992634 \n",
      "Epoch: 482 | MAE Train Loss: 6.0294406314344755 current Weight -> [1.35887293 2.10891831] ,current bias -> 0.6738020275406704 \n",
      "Epoch: 483 | MAE Train Loss: 6.003843326444894 current Weight -> [1.36168752 2.113303  ] ,current bias -> 0.6749006181924067 \n",
      "Epoch: 484 | MAE Train Loss: 5.97831461103423 current Weight -> [1.36450212 2.11768776] ,current bias -> 0.6759981798193632 \n",
      "Epoch: 485 | MAE Train Loss: 5.952854479837069 current Weight -> [1.36731672 2.12207258] ,current bias -> 0.6770947134853303 \n",
      "Epoch: 486 | MAE Train Loss: 5.927462927501027 current Weight -> [1.37013133 2.12645747] ,current bias -> 0.6781902202529986 \n",
      "Epoch: 487 | MAE Train Loss: 5.9021399486867185 current Weight -> [1.37294594 2.13084241] ,current bias -> 0.6792847011839599 \n",
      "Epoch: 488 | MAE Train Loss: 5.876885538067734 current Weight -> [1.37576055 2.13522743] ,current bias -> 0.6803781573387087 \n",
      "Epoch: 489 | MAE Train Loss: 5.851699690330593 current Weight -> [1.37857517 2.1396125 ] ,current bias -> 0.6814705897766431 \n",
      "Epoch: 490 | MAE Train Loss: 5.826582400174727 current Weight -> [1.3813898  2.14399764] ,current bias -> 0.6825619995560661 \n",
      "Epoch: 491 | MAE Train Loss: 5.801533662312453 current Weight -> [1.38420442 2.14838285] ,current bias -> 0.6836523877341867 \n",
      "Epoch: 492 | MAE Train Loss: 5.776553471468935 current Weight -> [1.38701905 2.15276811] ,current bias -> 0.6847417553671207 \n",
      "Epoch: 493 | MAE Train Loss: 5.751641822382157 current Weight -> [1.38983369 2.15715344] ,current bias -> 0.6858301035098923 \n",
      "Epoch: 494 | MAE Train Loss: 5.7267987098028925 current Weight -> [1.39264833 2.16153884] ,current bias -> 0.6869174332164351 \n",
      "Epoch: 495 | MAE Train Loss: 5.702024128494681 current Weight -> [1.39546297 2.16592429] ,current bias -> 0.688003745539593 \n",
      "Epoch: 496 | MAE Train Loss: 5.677318073233785 current Weight -> [1.39827762 2.17030981] ,current bias -> 0.6890890415311215 \n",
      "Epoch: 497 | MAE Train Loss: 5.652680538809176 current Weight -> [1.40109227 2.17469539] ,current bias -> 0.6901733222416889 \n",
      "Epoch: 498 | MAE Train Loss: 5.628111520022504 current Weight -> [1.40390693 2.17908104] ,current bias -> 0.6912565887208774 \n",
      "Epoch: 499 | MAE Train Loss: 5.603611011688051 current Weight -> [1.40672159 2.18346675] ,current bias -> 0.6923388420171839 \n",
      "Epoch: 500 | MAE Train Loss: 5.579179008632719 current Weight -> [1.40953625 2.18785252] ,current bias -> 0.6934200831780216 \n",
      "Epoch: 501 | MAE Train Loss: 5.554815505695997 current Weight -> [1.41235092 2.19223835] ,current bias -> 0.694500313249721 \n",
      "Epoch: 502 | MAE Train Loss: 5.530520497729928 current Weight -> [1.41516559 2.19662425] ,current bias -> 0.6955795332775304 \n",
      "Epoch: 503 | MAE Train Loss: 5.506293979599081 current Weight -> [1.41798027 2.20101021] ,current bias -> 0.6966577443056182 \n",
      "Epoch: 504 | MAE Train Loss: 5.482135946180532 current Weight -> [1.42079495 2.20539623] ,current bias -> 0.6977349473770729 \n",
      "Epoch: 505 | MAE Train Loss: 5.458046392363813 current Weight -> [1.42360963 2.20978231] ,current bias -> 0.6988111435339048 \n",
      "Epoch: 506 | MAE Train Loss: 5.43402531305091 current Weight -> [1.42642432 2.21416846] ,current bias -> 0.6998863338170471 \n",
      "Epoch: 507 | MAE Train Loss: 5.410072703156214 current Weight -> [1.42923901 2.21855467] ,current bias -> 0.7009605192663567 \n",
      "Epoch: 508 | MAE Train Loss: 5.386188557606495 current Weight -> [1.43205371 2.22294094] ,current bias -> 0.7020337009206157 \n",
      "Epoch: 509 | MAE Train Loss: 5.362372871340889 current Weight -> [1.43486841 2.22732728] ,current bias -> 0.7031058798175321 \n",
      "Epoch: 510 | MAE Train Loss: 5.338625639310851 current Weight -> [1.43768312 2.23171367] ,current bias -> 0.7041770569937416 \n",
      "Epoch: 511 | MAE Train Loss: 5.314946856480135 current Weight -> [1.44049782 2.23610013] ,current bias -> 0.7052472334848076 \n",
      "Epoch: 512 | MAE Train Loss: 5.291336517824765 current Weight -> [1.44331254 2.24048665] ,current bias -> 0.7063164103252233 \n",
      "Epoch: 513 | MAE Train Loss: 5.267794618333005 current Weight -> [1.44612725 2.24487323] ,current bias -> 0.7073845885484125 \n",
      "Epoch: 514 | MAE Train Loss: 5.244321153005328 current Weight -> [1.44894197 2.24925988] ,current bias -> 0.7084517691867305 \n",
      "Epoch: 515 | MAE Train Loss: 5.2209161168544 current Weight -> [1.4517567  2.25364658] ,current bias -> 0.7095179532714655 \n",
      "Epoch: 516 | MAE Train Loss: 5.197579504905035 current Weight -> [1.45457143 2.25803335] ,current bias -> 0.7105831418328395 \n",
      "Epoch: 517 | MAE Train Loss: 5.17431131219418 current Weight -> [1.45738616 2.26242018] ,current bias -> 0.7116473359000095 \n",
      "Epoch: 518 | MAE Train Loss: 5.151111533770879 current Weight -> [1.4602009  2.26680707] ,current bias -> 0.7127105365010685 \n",
      "Epoch: 519 | MAE Train Loss: 5.12798016469625 current Weight -> [1.46301564 2.27119402] ,current bias -> 0.7137727446630469 \n",
      "Epoch: 520 | MAE Train Loss: 5.104917200043452 current Weight -> [1.46583038 2.27558104] ,current bias -> 0.714833961411913 \n",
      "Epoch: 521 | MAE Train Loss: 5.081922634897666 current Weight -> [1.46864513 2.27996811] ,current bias -> 0.7158941877725749 \n",
      "Epoch: 522 | MAE Train Loss: 5.058996464356053 current Weight -> [1.47145988 2.28435525] ,current bias -> 0.7169534247688807 \n",
      "Epoch: 523 | MAE Train Loss: 5.036138683527742 current Weight -> [1.47427464 2.28874245] ,current bias -> 0.7180116734236204 \n",
      "Epoch: 524 | MAE Train Loss: 5.01334928753379 current Weight -> [1.4770894  2.29312971] ,current bias -> 0.7190689347585266 \n",
      "Epoch: 525 | MAE Train Loss: 4.9906282715071635 current Weight -> [1.47990416 2.29751703] ,current bias -> 0.7201252097942756 \n",
      "Epoch: 526 | MAE Train Loss: 4.967975630592702 current Weight -> [1.48271893 2.30190441] ,current bias -> 0.7211804995504886 \n",
      "Epoch: 527 | MAE Train Loss: 4.9453913599471 current Weight -> [1.4855337  2.30629185] ,current bias -> 0.7222348050457325 \n",
      "Epoch: 528 | MAE Train Loss: 4.922875454738872 current Weight -> [1.48834848 2.31067936] ,current bias -> 0.7232881272975215 \n",
      "Epoch: 529 | MAE Train Loss: 4.900427910148328 current Weight -> [1.49116326 2.31506692] ,current bias -> 0.7243404673223177 \n",
      "Epoch: 530 | MAE Train Loss: 4.878048721367545 current Weight -> [1.49397805 2.31945455] ,current bias -> 0.7253918261355328 \n",
      "Epoch: 531 | MAE Train Loss: 4.855737883600344 current Weight -> [1.49679283 2.32384224] ,current bias -> 0.7264422047515282 \n",
      "Epoch: 532 | MAE Train Loss: 4.833495392062259 current Weight -> [1.49960763 2.32822998] ,current bias -> 0.7274916041836171 \n",
      "Epoch: 533 | MAE Train Loss: 4.811321241980507 current Weight -> [1.50242242 2.33261779] ,current bias -> 0.7285400254440649 \n",
      "Epoch: 534 | MAE Train Loss: 4.78921542859397 current Weight -> [1.50523722 2.33700566] ,current bias -> 0.7295874695440908 \n",
      "Epoch: 535 | MAE Train Loss: 4.767177947153153 current Weight -> [1.50805203 2.34139359] ,current bias -> 0.7306339374938683 \n",
      "Epoch: 536 | MAE Train Loss: 4.745208792920177 current Weight -> [1.51086683 2.34578158] ,current bias -> 0.7316794303025268 \n",
      "Epoch: 537 | MAE Train Loss: 4.723307961168738 current Weight -> [1.51368165 2.35016963] ,current bias -> 0.7327239489781525 \n",
      "Epoch: 538 | MAE Train Loss: 4.701475447184081 current Weight -> [1.51649646 2.35455774] ,current bias -> 0.7337674945277893 \n",
      "Epoch: 539 | MAE Train Loss: 4.679711246262977 current Weight -> [1.51931128 2.35894592] ,current bias -> 0.7348100679574402 \n",
      "Epoch: 540 | MAE Train Loss: 4.658015353713698 current Weight -> [1.5221261  2.36333415] ,current bias -> 0.7358516702720681 \n",
      "Epoch: 541 | MAE Train Loss: 4.636387764855984 current Weight -> [1.52494093 2.36772244] ,current bias -> 0.736892302475597 \n",
      "Epoch: 542 | MAE Train Loss: 4.614828475021021 current Weight -> [1.52775576 2.37211079] ,current bias -> 0.7379319655709131 \n",
      "Epoch: 543 | MAE Train Loss: 4.593337479551415 current Weight -> [1.5305706  2.37649921] ,current bias -> 0.7389706605598658 \n",
      "Epoch: 544 | MAE Train Loss: 4.5719147738011605 current Weight -> [1.53338544 2.38088768] ,current bias -> 0.7400083884432689 \n",
      "Epoch: 545 | MAE Train Loss: 4.550560353135621 current Weight -> [1.53620028 2.38527621] ,current bias -> 0.7410451502209013 \n",
      "Epoch: 546 | MAE Train Loss: 4.529274212931499 current Weight -> [1.53901513 2.3896648 ] ,current bias -> 0.7420809468915087 \n",
      "Epoch: 547 | MAE Train Loss: 4.5080563485768055 current Weight -> [1.54182998 2.39405345] ,current bias -> 0.7431157794528042 \n",
      "Epoch: 548 | MAE Train Loss: 4.4869067554708435 current Weight -> [1.54464483 2.39844217] ,current bias -> 0.7441496489014692 \n",
      "Epoch: 549 | MAE Train Loss: 4.4658254290241715 current Weight -> [1.54745969 2.40283094] ,current bias -> 0.745182556233155 \n",
      "Epoch: 550 | MAE Train Loss: 4.4448123646585875 current Weight -> [1.55027455 2.40721977] ,current bias -> 0.7462145024424837 \n",
      "Epoch: 551 | MAE Train Loss: 4.4238675578070925 current Weight -> [1.55308942 2.41160866] ,current bias -> 0.747245488523049 \n",
      "Epoch: 552 | MAE Train Loss: 4.402991003913873 current Weight -> [1.55590429 2.41599761] ,current bias -> 0.7482755154674174 \n",
      "Epoch: 553 | MAE Train Loss: 4.382182698434271 current Weight -> [1.55871916 2.42038662] ,current bias -> 0.7493045842671295 \n",
      "Epoch: 554 | MAE Train Loss: 4.361442636834758 current Weight -> [1.56153404 2.42477569] ,current bias -> 0.7503326959127008 \n",
      "Epoch: 555 | MAE Train Loss: 4.340770814592912 current Weight -> [1.56434892 2.42916482] ,current bias -> 0.7513598513936226 \n",
      "Epoch: 556 | MAE Train Loss: 4.320167227197385 current Weight -> [1.56716381 2.43355401] ,current bias -> 0.7523860516983636 \n",
      "Epoch: 557 | MAE Train Loss: 4.299631870147886 current Weight -> [1.5699787  2.43794325] ,current bias -> 0.7534112978143703 \n",
      "Epoch: 558 | MAE Train Loss: 4.279164738955151 current Weight -> [1.57279359 2.44233256] ,current bias -> 0.7544355907280688 \n",
      "Epoch: 559 | MAE Train Loss: 4.258765829140917 current Weight -> [1.57560849 2.44672193] ,current bias -> 0.7554589314248651 \n",
      "Epoch: 560 | MAE Train Loss: 4.238435136237895 current Weight -> [1.57842339 2.45111135] ,current bias -> 0.7564813208891467 \n",
      "Epoch: 561 | MAE Train Loss: 4.2181726557897505 current Weight -> [1.58123829 2.45550083] ,current bias -> 0.7575027601042832 \n",
      "Epoch: 562 | MAE Train Loss: 4.1979783833510735 current Weight -> [1.5840532  2.45989038] ,current bias -> 0.758523250052628 \n",
      "Epoch: 563 | MAE Train Loss: 4.177852314487349 current Weight -> [1.58686812 2.46427998] ,current bias -> 0.7595427917155186 \n",
      "Epoch: 564 | MAE Train Loss: 4.1577944447749395 current Weight -> [1.58968303 2.46866964] ,current bias -> 0.7605613860732783 \n",
      "Epoch: 565 | MAE Train Loss: 4.137804769801059 current Weight -> [1.59249795 2.47305936] ,current bias -> 0.7615790341052167 \n",
      "Epoch: 566 | MAE Train Loss: 4.117883285163746 current Weight -> [1.59531287 2.47744914] ,current bias -> 0.7625957367896311 \n",
      "Epoch: 567 | MAE Train Loss: 4.098029986471828 current Weight -> [1.5981278  2.48183897] ,current bias -> 0.7636114951038075 \n",
      "Epoch: 568 | MAE Train Loss: 4.07824486934492 current Weight -> [1.60094273 2.48622887] ,current bias -> 0.7646263100240217 \n",
      "Epoch: 569 | MAE Train Loss: 4.058527929413377 current Weight -> [1.60375767 2.49061882] ,current bias -> 0.7656401825255399 \n",
      "Epoch: 570 | MAE Train Loss: 4.038879162318279 current Weight -> [1.60657261 2.49500883] ,current bias -> 0.7666531135826203 \n",
      "Epoch: 571 | MAE Train Loss: 4.019298563711406 current Weight -> [1.60938755 2.4993989 ] ,current bias -> 0.7676651041685141 \n",
      "Epoch: 572 | MAE Train Loss: 3.9997861292552086 current Weight -> [1.6122025  2.50378903] ,current bias -> 0.768676155255466 \n",
      "Epoch: 573 | MAE Train Loss: 3.9803418546227936 current Weight -> [1.61501745 2.50817922] ,current bias -> 0.7696862678147158 \n",
      "Epoch: 574 | MAE Train Loss: 3.960965735497881 current Weight -> [1.6178324  2.51256946] ,current bias -> 0.7706954428164994 \n",
      "Epoch: 575 | MAE Train Loss: 3.941657767574802 current Weight -> [1.62064736 2.51695977] ,current bias -> 0.7717036812300494 \n",
      "Epoch: 576 | MAE Train Loss: 3.9224179465584545 current Weight -> [1.62346232 2.52135013] ,current bias -> 0.7727109840235966 \n",
      "Epoch: 577 | MAE Train Loss: 3.9032462681642897 current Weight -> [1.62627729 2.52574055] ,current bias -> 0.7737173521643708 \n",
      "Epoch: 578 | MAE Train Loss: 3.884142728118283 current Weight -> [1.62909225 2.53013102] ,current bias -> 0.7747227866186018 \n",
      "Epoch: 579 | MAE Train Loss: 3.8651073221569137 current Weight -> [1.63190723 2.53452156] ,current bias -> 0.7757272883515207 \n",
      "Epoch: 580 | MAE Train Loss: 3.8461400460271333 current Weight -> [1.6347222  2.53891215] ,current bias -> 0.7767308583273607 \n",
      "Epoch: 581 | MAE Train Loss: 3.8272408954863524 current Weight -> [1.63753718 2.5433028 ] ,current bias -> 0.777733497509358 \n",
      "Epoch: 582 | MAE Train Loss: 3.8084098663024 current Weight -> [1.64035217 2.54769351] ,current bias -> 0.7787352068597535 \n",
      "Epoch: 583 | MAE Train Loss: 3.7896469542535205 current Weight -> [1.64316716 2.55208428] ,current bias -> 0.7797359873397929 \n",
      "Epoch: 584 | MAE Train Loss: 3.770952155128327 current Weight -> [1.64598215 2.5564751 ] ,current bias -> 0.7807358399097283 \n",
      "Epoch: 585 | MAE Train Loss: 3.7523254647257955 current Weight -> [1.64879714 2.56086598] ,current bias -> 0.781734765528819 \n",
      "Epoch: 586 | MAE Train Loss: 3.7337668788552274 current Weight -> [1.65161214 2.56525692] ,current bias -> 0.782732765155333 \n",
      "Epoch: 587 | MAE Train Loss: 3.715276393336236 current Weight -> [1.65442714 2.56964792] ,current bias -> 0.7837298397465472 \n",
      "Epoch: 588 | MAE Train Loss: 3.696854003998713 current Weight -> [1.65724215 2.57403897] ,current bias -> 0.7847259902587492 \n",
      "Epoch: 589 | MAE Train Loss: 3.6784997066828153 current Weight -> [1.66005716 2.57843008] ,current bias -> 0.785721217647238 \n",
      "Epoch: 590 | MAE Train Loss: 3.6602134972389306 current Weight -> [1.66287217 2.58282125] ,current bias -> 0.7867155228663247 \n",
      "Epoch: 591 | MAE Train Loss: 3.641995371527658 current Weight -> [1.66568719 2.58721248] ,current bias -> 0.7877089068693343 \n",
      "Epoch: 592 | MAE Train Loss: 3.6238453254197873 current Weight -> [1.66850221 2.59160376] ,current bias -> 0.7887013706086059 \n",
      "Epoch: 593 | MAE Train Loss: 3.605763354796268 current Weight -> [1.67131724 2.5959951 ] ,current bias -> 0.7896929150354943 \n",
      "Epoch: 594 | MAE Train Loss: 3.587749455548193 current Weight -> [1.67413227 2.60038649] ,current bias -> 0.7906835411003708 \n",
      "Epoch: 595 | MAE Train Loss: 3.5698036235767687 current Weight -> [1.6769473  2.60477795] ,current bias -> 0.7916732497526239 \n",
      "Epoch: 596 | MAE Train Loss: 3.5519258547932986 current Weight -> [1.67976234 2.60916946] ,current bias -> 0.792662041940661 \n",
      "Epoch: 597 | MAE Train Loss: 3.5341161451191527 current Weight -> [1.68257738 2.61356102] ,current bias -> 0.7936499186119091 \n",
      "Epoch: 598 | MAE Train Loss: 3.516374490485748 current Weight -> [1.68539242 2.61795265] ,current bias -> 0.7946368807128156 \n",
      "Epoch: 599 | MAE Train Loss: 3.498700886834523 current Weight -> [1.68820747 2.62234433] ,current bias -> 0.7956229291888492 \n",
      "Epoch: 600 | MAE Train Loss: 3.4810953301169136 current Weight -> [1.69102252 2.62673607] ,current bias -> 0.7966080649845018 \n",
      "Epoch: 601 | MAE Train Loss: 3.463557816294335 current Weight -> [1.69383757 2.63112786] ,current bias -> 0.7975922890432883 \n",
      "Epoch: 602 | MAE Train Loss: 3.4460883413381556 current Weight -> [1.69665263 2.63551971] ,current bias -> 0.7985756023077486 \n",
      "Epoch: 603 | MAE Train Loss: 3.428686901229669 current Weight -> [1.69946769 2.63991162] ,current bias -> 0.7995580057194481 \n",
      "Epoch: 604 | MAE Train Loss: 3.4113534919600776 current Weight -> [1.70228276 2.64430358] ,current bias -> 0.8005395002189788 \n",
      "Epoch: 605 | MAE Train Loss: 3.3940881095304665 current Weight -> [1.70509782 2.6486956 ] ,current bias -> 0.8015200867459603 \n",
      "Epoch: 606 | MAE Train Loss: 3.3768907499517797 current Weight -> [1.7079129  2.65308768] ,current bias -> 0.8024997662390411 \n",
      "Epoch: 607 | MAE Train Loss: 3.359761409244797 current Weight -> [1.71072797 2.65747981] ,current bias -> 0.8034785396358989 \n",
      "Epoch: 608 | MAE Train Loss: 3.342700083440115 current Weight -> [1.71354305 2.661872  ] ,current bias -> 0.8044564078732422 \n",
      "Epoch: 609 | MAE Train Loss: 3.3257067685781214 current Weight -> [1.71635814 2.66626424] ,current bias -> 0.8054333718868115 \n",
      "Epoch: 610 | MAE Train Loss: 3.308781460708969 current Weight -> [1.71917322 2.67065654] ,current bias -> 0.8064094326113794 \n",
      "Epoch: 611 | MAE Train Loss: 3.2919241558925605 current Weight -> [1.72198832 2.6750489 ] ,current bias -> 0.8073845909807525 \n",
      "Epoch: 612 | MAE Train Loss: 3.2751348501985174 current Weight -> [1.72480341 2.67944131] ,current bias -> 0.808358847927772 \n",
      "Epoch: 613 | MAE Train Loss: 3.258413539706159 current Weight -> [1.72761851 2.68383378] ,current bias -> 0.8093322043843146 \n",
      "Epoch: 614 | MAE Train Loss: 3.241760220504489 current Weight -> [1.73043361 2.68822631] ,current bias -> 0.8103046612812939 \n",
      "Epoch: 615 | MAE Train Loss: 3.225174888692158 current Weight -> [1.73324872 2.69261889] ,current bias -> 0.8112762195486607 \n",
      "Epoch: 616 | MAE Train Loss: 3.208657540377456 current Weight -> [1.73606382 2.69701152] ,current bias -> 0.8122468801154048 \n",
      "Epoch: 617 | MAE Train Loss: 3.192208171678277 current Weight -> [1.73887894 2.70140422] ,current bias -> 0.8132166439095556 \n",
      "Epoch: 618 | MAE Train Loss: 3.1758267787221057 current Weight -> [1.74169405 2.70579696] ,current bias -> 0.814185511858183 \n",
      "Epoch: 619 | MAE Train Loss: 3.159513357645987 current Weight -> [1.74450917 2.71018977] ,current bias -> 0.8151534848873986 \n",
      "Epoch: 620 | MAE Train Loss: 3.1432679045965126 current Weight -> [1.7473243  2.71458262] ,current bias -> 0.8161205639223564 \n",
      "Epoch: 621 | MAE Train Loss: 3.127090415729791 current Weight -> [1.75013942 2.71897554] ,current bias -> 0.8170867498872543 \n",
      "Epoch: 622 | MAE Train Loss: 3.1109808872114324 current Weight -> [1.75295455 2.72336851] ,current bias -> 0.8180520437053345 \n",
      "Epoch: 623 | MAE Train Loss: 3.0949393152165188 current Weight -> [1.75576969 2.72776153] ,current bias -> 0.8190164462988849 \n",
      "Epoch: 624 | MAE Train Loss: 3.0789656959295866 current Weight -> [1.75858483 2.73215461] ,current bias -> 0.81997995858924 \n",
      "Epoch: 625 | MAE Train Loss: 3.0630600255446057 current Weight -> [1.76139997 2.73654775] ,current bias -> 0.8209425814967817 \n",
      "Epoch: 626 | MAE Train Loss: 3.047222300264951 current Weight -> [1.76421511 2.74094094] ,current bias -> 0.8219043159409405 \n",
      "Epoch: 627 | MAE Train Loss: 3.0314525163033874 current Weight -> [1.76703026 2.74533418] ,current bias -> 0.8228651628401965 \n",
      "Epoch: 628 | MAE Train Loss: 3.015750669882046 current Weight -> [1.76984541 2.74972748] ,current bias -> 0.8238251231120801 \n",
      "Epoch: 629 | MAE Train Loss: 3.0001167572323975 current Weight -> [1.77266057 2.75412084] ,current bias -> 0.8247841976731733 \n",
      "Epoch: 630 | MAE Train Loss: 2.984550774595239 current Weight -> [1.77547573 2.75851425] ,current bias -> 0.8257423874391105 \n",
      "Epoch: 631 | MAE Train Loss: 2.969052718220664 current Weight -> [1.77829089 2.76290771] ,current bias -> 0.8266996933245796 \n",
      "Epoch: 632 | MAE Train Loss: 2.953622584368043 current Weight -> [1.78110606 2.76730123] ,current bias -> 0.8276561162433228 \n",
      "Epoch: 633 | MAE Train Loss: 2.9382603693060054 current Weight -> [1.78392123 2.77169481] ,current bias -> 0.8286116571081377 \n",
      "Epoch: 634 | MAE Train Loss: 2.9229660693124146 current Weight -> [1.7867364  2.77608844] ,current bias -> 0.8295663168308783 \n",
      "Epoch: 635 | MAE Train Loss: 2.907739680674345 current Weight -> [1.78955158 2.78048212] ,current bias -> 0.830520096322456 \n",
      "Epoch: 636 | MAE Train Loss: 2.892581199688064 current Weight -> [1.79236676 2.78487586] ,current bias -> 0.8314729964928406 \n",
      "Epoch: 637 | MAE Train Loss: 2.877490622659009 current Weight -> [1.79518194 2.78926965] ,current bias -> 0.8324250182510607 \n",
      "Epoch: 638 | MAE Train Loss: 2.862467945901766 current Weight -> [1.79799713 2.7936635 ] ,current bias -> 0.8333761625052059 \n",
      "Epoch: 639 | MAE Train Loss: 2.847513165740046 current Weight -> [1.80081232 2.7980574 ] ,current bias -> 0.8343264301624264 \n",
      "Epoch: 640 | MAE Train Loss: 2.8326262785066674 current Weight -> [1.80362751 2.80245136] ,current bias -> 0.8352758221289348 \n",
      "Epoch: 641 | MAE Train Loss: 2.817807280543529 current Weight -> [1.80644271 2.80684537] ,current bias -> 0.8362243393100071 \n",
      "Epoch: 642 | MAE Train Loss: 2.8030561682015973 current Weight -> [1.80925791 2.81123943] ,current bias -> 0.837171982609983 \n",
      "Epoch: 643 | MAE Train Loss: 2.7883729378408812 current Weight -> [1.81207312 2.81563355] ,current bias -> 0.8381187529322678 \n",
      "Epoch: 644 | MAE Train Loss: 2.7737575858304035 current Weight -> [1.81488833 2.82002773] ,current bias -> 0.8390646511793325 \n",
      "Epoch: 645 | MAE Train Loss: 2.759210108548192 current Weight -> [1.81770354 2.82442195] ,current bias -> 0.8400096782527152 \n",
      "Epoch: 646 | MAE Train Loss: 2.7447305023812514 current Weight -> [1.82051875 2.82881623] ,current bias -> 0.8409538350530222 \n",
      "Epoch: 647 | MAE Train Loss: 2.7303187637255433 current Weight -> [1.82333397 2.83321057] ,current bias -> 0.8418971224799287 \n",
      "Epoch: 648 | MAE Train Loss: 2.7159748889859645 current Weight -> [1.8261492  2.83760496] ,current bias -> 0.8428395414321795 \n",
      "Epoch: 649 | MAE Train Loss: 2.701698874576329 current Weight -> [1.82896442 2.8419994 ] ,current bias -> 0.8437810928075908 \n",
      "Epoch: 650 | MAE Train Loss: 2.6874907169193425 current Weight -> [1.83177965 2.8463939 ] ,current bias -> 0.8447217775030504 \n",
      "Epoch: 651 | MAE Train Loss: 2.6733504124465877 current Weight -> [1.83459488 2.85078845] ,current bias -> 0.8456615964145189 \n",
      "Epoch: 652 | MAE Train Loss: 2.6592779575984973 current Weight -> [1.83741012 2.85518305] ,current bias -> 0.8466005504370305 \n",
      "Epoch: 653 | MAE Train Loss: 2.6452733488243365 current Weight -> [1.84022536 2.85957771] ,current bias -> 0.8475386404646946 \n",
      "Epoch: 654 | MAE Train Loss: 2.6313365825821813 current Weight -> [1.8430406  2.86397242] ,current bias -> 0.8484758673906958 \n",
      "Epoch: 655 | MAE Train Loss: 2.617467655338901 current Weight -> [1.84585585 2.86836719] ,current bias -> 0.8494122321072957 \n",
      "Epoch: 656 | MAE Train Loss: 2.603666563570128 current Weight -> [1.8486711  2.87276201] ,current bias -> 0.8503477355058332 \n",
      "Epoch: 657 | MAE Train Loss: 2.5899333037602528 current Weight -> [1.85148636 2.87715688] ,current bias -> 0.8512823784767259 \n",
      "Epoch: 658 | MAE Train Loss: 2.576267872402388 current Weight -> [1.85430161 2.88155181] ,current bias -> 0.8522161619094709 \n",
      "Epoch: 659 | MAE Train Loss: 2.5626702659983573 current Weight -> [1.85711687 2.88594678] ,current bias -> 0.8531490866926458 \n",
      "Epoch: 660 | MAE Train Loss: 2.5491404810586724 current Weight -> [1.85993214 2.89034182] ,current bias -> 0.8540811537139095 \n",
      "Epoch: 661 | MAE Train Loss: 2.5356785141025107 current Weight -> [1.86274741 2.8947369 ] ,current bias -> 0.8550123638600031 \n",
      "Epoch: 662 | MAE Train Loss: 2.5222843616576984 current Weight -> [1.86556268 2.89913204] ,current bias -> 0.8559427180167515 \n",
      "Epoch: 663 | MAE Train Loss: 2.508958020260687 current Weight -> [1.86837795 2.90352723] ,current bias -> 0.8568722170690634 \n",
      "Epoch: 664 | MAE Train Loss: 2.495699486456537 current Weight -> [1.87119323 2.90792248] ,current bias -> 0.8578008619009327 \n",
      "Epoch: 665 | MAE Train Loss: 2.482508756798891 current Weight -> [1.87400851 2.91231777] ,current bias -> 0.8587286533954398 \n",
      "Epoch: 666 | MAE Train Loss: 2.4693858278499614 current Weight -> [1.8768238  2.91671312] ,current bias -> 0.8596555924347518 \n",
      "Epoch: 667 | MAE Train Loss: 2.4563306961805056 current Weight -> [1.87963908 2.92110853] ,current bias -> 0.8605816799001239 \n",
      "Epoch: 668 | MAE Train Loss: 2.4433433583698068 current Weight -> [1.88245438 2.92550398] ,current bias -> 0.8615069166719006 \n",
      "Epoch: 669 | MAE Train Loss: 2.430423811005652 current Weight -> [1.88526967 2.92989949] ,current bias -> 0.8624313036295159 \n",
      "Epoch: 670 | MAE Train Loss: 2.417572050684318 current Weight -> [1.88808497 2.93429505] ,current bias -> 0.8633548416514949 \n",
      "Epoch: 671 | MAE Train Loss: 2.404788074010545 current Weight -> [1.89090027 2.93869067] ,current bias -> 0.8642775316154543 \n",
      "Epoch: 672 | MAE Train Loss: 2.3920718775975205 current Weight -> [1.89371558 2.94308633] ,current bias -> 0.8651993743981038 \n",
      "Epoch: 673 | MAE Train Loss: 2.3794234580668556 current Weight -> [1.89653089 2.94748205] ,current bias -> 0.8661203708752465 \n",
      "Epoch: 674 | MAE Train Loss: 2.36684281204857 current Weight -> [1.8993462  2.95187782] ,current bias -> 0.8670405219217803 \n",
      "Epoch: 675 | MAE Train Loss: 2.35432993618107 current Weight -> [1.90216151 2.95627365] ,current bias -> 0.8679598284116985 \n",
      "Epoch: 676 | MAE Train Loss: 2.34188482711113 current Weight -> [1.90497683 2.96066952] ,current bias -> 0.868878291218091 \n",
      "Epoch: 677 | MAE Train Loss: 2.3295074814938674 current Weight -> [1.90779215 2.96506545] ,current bias -> 0.8697959112131451 \n",
      "Epoch: 678 | MAE Train Loss: 2.3171978959927326 current Weight -> [1.91060748 2.96946143] ,current bias -> 0.8707126892681464 \n",
      "Epoch: 679 | MAE Train Loss: 2.304956067279478 current Weight -> [1.91342281 2.97385747] ,current bias -> 0.8716286262534797 \n",
      "Epoch: 680 | MAE Train Loss: 2.29278199203415 current Weight -> [1.91623814 2.97825355] ,current bias -> 0.8725437230386303 \n",
      "Epoch: 681 | MAE Train Loss: 2.2806756669450596 current Weight -> [1.91905348 2.98264969] ,current bias -> 0.8734579804921841 \n",
      "Epoch: 682 | MAE Train Loss: 2.268637088708771 current Weight -> [1.92186882 2.98704588] ,current bias -> 0.8743713994818297 \n",
      "Epoch: 683 | MAE Train Loss: 2.256666254030076 current Weight -> [1.92468416 2.99144212] ,current bias -> 0.8752839808743583 \n",
      "Epoch: 684 | MAE Train Loss: 2.2447631596219777 current Weight -> [1.92749951 2.99583842] ,current bias -> 0.8761957255356652 \n",
      "Epoch: 685 | MAE Train Loss: 2.23292780220567 current Weight -> [1.93031486 3.00023476] ,current bias -> 0.8771066343307503 \n",
      "Epoch: 686 | MAE Train Loss: 2.221160178510519 current Weight -> [1.93313021 3.00463116] ,current bias -> 0.8780167081237197 \n",
      "Epoch: 687 | MAE Train Loss: 2.2094602852740435 current Weight -> [1.93594556 3.00902761] ,current bias -> 0.8789259477777858 \n",
      "Epoch: 688 | MAE Train Loss: 2.1978281192418985 current Weight -> [1.93876092 3.01342411] ,current bias -> 0.8798343541552687 \n",
      "Epoch: 689 | MAE Train Loss: 2.1862636771678505 current Weight -> [1.94157629 3.01782066] ,current bias -> 0.8807419281175972 \n",
      "Epoch: 690 | MAE Train Loss: 2.1747669558137606 current Weight -> [1.94439165 3.02221726] ,current bias -> 0.8816486705253096 \n",
      "Epoch: 691 | MAE Train Loss: 2.16333795194957 current Weight -> [1.94720702 3.02661392] ,current bias -> 0.8825545822380544 \n",
      "Epoch: 692 | MAE Train Loss: 2.1519766623532735 current Weight -> [1.95002239 3.03101063] ,current bias -> 0.8834596641145914 \n",
      "Epoch: 693 | MAE Train Loss: 2.1406830838109054 current Weight -> [1.95283777 3.03540739] ,current bias -> 0.8843639170127929 \n",
      "Epoch: 694 | MAE Train Loss: 2.1294572131165195 current Weight -> [1.95565315 3.0398042 ] ,current bias -> 0.8852673417896441 \n",
      "Epoch: 695 | MAE Train Loss: 2.118299047072172 current Weight -> [1.95846853 3.04420106] ,current bias -> 0.8861699393012442 \n",
      "Epoch: 696 | MAE Train Loss: 2.107208582487896 current Weight -> [1.96128392 3.04859797] ,current bias -> 0.8870717104028076 \n",
      "Epoch: 697 | MAE Train Loss: 2.0961858161816918 current Weight -> [1.96409931 3.05299493] ,current bias -> 0.8879726559486645 \n",
      "Epoch: 698 | MAE Train Loss: 2.0852307449795005 current Weight -> [1.9669147  3.05739195] ,current bias -> 0.8888727767922618 \n",
      "Epoch: 699 | MAE Train Loss: 2.0743433657151904 current Weight -> [1.9697301  3.06178902] ,current bias -> 0.8897720737861644 \n",
      "Epoch: 700 | MAE Train Loss: 2.0635236752305373 current Weight -> [1.9725455  3.06618613] ,current bias -> 0.8906705477820555 \n",
      "Epoch: 701 | MAE Train Loss: 2.0527716703752006 current Weight -> [1.9753609 3.0705833] ,current bias -> 0.891568199630738 \n",
      "Epoch: 702 | MAE Train Loss: 2.0420873480067128 current Weight -> [1.97817631 3.07498052] ,current bias -> 0.8924650301821353 \n",
      "Epoch: 703 | MAE Train Loss: 2.0314707049904577 current Weight -> [1.98099172 3.07937779] ,current bias -> 0.8933610402852921 \n",
      "Epoch: 704 | MAE Train Loss: 2.020921738199648 current Weight -> [1.98380713 3.08377511] ,current bias -> 0.8942562307883754 \n",
      "Epoch: 705 | MAE Train Loss: 2.0104404445153112 current Weight -> [1.98662254 3.08817249] ,current bias -> 0.8951506025386753 \n",
      "Epoch: 706 | MAE Train Loss: 2.000026820826273 current Weight -> [1.98943796 3.09256991] ,current bias -> 0.896044156382606 \n",
      "Epoch: 707 | MAE Train Loss: 1.9896808640291312 current Weight -> [1.99225339 3.09696738] ,current bias -> 0.8969368931657069 \n",
      "Epoch: 708 | MAE Train Loss: 1.9794025710282466 current Weight -> [1.99506881 3.10136491] ,current bias -> 0.897828813732643 \n",
      "Epoch: 709 | MAE Train Loss: 1.9691919387357177 current Weight -> [1.99788424 3.10576248] ,current bias -> 0.8987199189272062 \n",
      "Epoch: 710 | MAE Train Loss: 1.9590489640713662 current Weight -> [2.00069967 3.11016011] ,current bias -> 0.8996102095923161 \n",
      "Epoch: 711 | MAE Train Loss: 1.9489736439627152 current Weight -> [2.00351511 3.11455778] ,current bias -> 0.900499686570021 \n",
      "Epoch: 712 | MAE Train Loss: 1.938965975344977 current Weight -> [2.00633055 3.11895551] ,current bias -> 0.9013883507014987 \n",
      "Epoch: 713 | MAE Train Loss: 1.9290259551610276 current Weight -> [2.00914599 3.12335329] ,current bias -> 0.9022762028270572 \n",
      "Epoch: 714 | MAE Train Loss: 1.9191535803613948 current Weight -> [2.01196143 3.12775111] ,current bias -> 0.903163243786136 \n",
      "Epoch: 715 | MAE Train Loss: 1.9093488479042369 current Weight -> [2.01477688 3.13214899] ,current bias -> 0.9040494744173067 \n",
      "Epoch: 716 | MAE Train Loss: 1.8996117547553246 current Weight -> [2.01759233 3.13654692] ,current bias -> 0.904934895558274 \n",
      "Epoch: 717 | MAE Train Loss: 1.889942297888025 current Weight -> [2.02040779 3.1409449 ] ,current bias -> 0.9058195080458767 \n",
      "Epoch: 718 | MAE Train Loss: 1.880340474283281 current Weight -> [2.02322325 3.14534292] ,current bias -> 0.9067033127160884 \n",
      "Epoch: 719 | MAE Train Loss: 1.8708062809295969 current Weight -> [2.02603871 3.149741  ] ,current bias -> 0.9075863104040185 \n",
      "Epoch: 720 | MAE Train Loss: 1.8613397148230157 current Weight -> [2.02885418 3.15413913] ,current bias -> 0.908468501943913 \n",
      "Epoch: 721 | MAE Train Loss: 1.8519407729671062 current Weight -> [2.03166964 3.15853731] ,current bias -> 0.9093498881691556 \n",
      "Epoch: 722 | MAE Train Loss: 1.842609452372943 current Weight -> [2.03448511 3.16293554] ,current bias -> 0.9102304699122684 \n",
      "Epoch: 723 | MAE Train Loss: 1.833345750059088 current Weight -> [2.03730059 3.16733381] ,current bias -> 0.9111102480049129 \n",
      "Epoch: 724 | MAE Train Loss: 1.824149663051573 current Weight -> [2.04011607 3.17173214] ,current bias -> 0.9119892232778908 \n",
      "Epoch: 725 | MAE Train Loss: 1.8150211883838838 current Weight -> [2.04293155 3.17613052] ,current bias -> 0.9128673965611448 \n",
      "Epoch: 726 | MAE Train Loss: 1.80596032309694 current Weight -> [2.04574703 3.18052895] ,current bias -> 0.9137447686837598 \n",
      "Epoch: 727 | MAE Train Loss: 1.7969670642390807 current Weight -> [2.04856252 3.18492743] ,current bias -> 0.9146213404739635 \n",
      "Epoch: 728 | MAE Train Loss: 1.7880414088660428 current Weight -> [2.05137801 3.18932595] ,current bias -> 0.9154971127591276 \n",
      "Epoch: 729 | MAE Train Loss: 1.7791833540409454 current Weight -> [2.0541935  3.19372453] ,current bias -> 0.9163720863657682 \n",
      "Epoch: 730 | MAE Train Loss: 1.770392896834275 current Weight -> [2.057009   3.19812315] ,current bias -> 0.9172462621195471 \n",
      "Epoch: 731 | MAE Train Loss: 1.7616700343238647 current Weight -> [2.0598245  3.20252183] ,current bias -> 0.9181196408452724 \n",
      "Epoch: 732 | MAE Train Loss: 1.7530147635948765 current Weight -> [2.06264001 3.20692055] ,current bias -> 0.9189922233668999 \n",
      "Epoch: 733 | MAE Train Loss: 1.7444270817397862 current Weight -> [2.06545551 3.21131933] ,current bias -> 0.9198640105075331 \n",
      "Epoch: 734 | MAE Train Loss: 1.7359069858583651 current Weight -> [2.06827102 3.21571815] ,current bias -> 0.9207350030894249 \n",
      "Epoch: 735 | MAE Train Loss: 1.727454473057663 current Weight -> [2.07108654 3.22011702] ,current bias -> 0.9216052019339782 \n",
      "Epoch: 736 | MAE Train Loss: 1.7190695404519898 current Weight -> [2.07390205 3.22451595] ,current bias -> 0.9224746078617466 \n",
      "Epoch: 737 | MAE Train Loss: 1.7107521851628997 current Weight -> [2.07671757 3.22891492] ,current bias -> 0.9233432216924355 \n",
      "Epoch: 738 | MAE Train Loss: 1.702502404319174 current Weight -> [2.07953309 3.23331394] ,current bias -> 0.9242110442449029 \n",
      "Epoch: 739 | MAE Train Loss: 1.6943201950568028 current Weight -> [2.08234862 3.23771301] ,current bias -> 0.9250780763371601 \n",
      "Epoch: 740 | MAE Train Loss: 1.6862055545189674 current Weight -> [2.08516415 3.24211213] ,current bias -> 0.9259443187863731 \n",
      "Epoch: 741 | MAE Train Loss: 1.6781584798560283 current Weight -> [2.08797968 3.24651129] ,current bias -> 0.9268097724088628 \n",
      "Epoch: 742 | MAE Train Loss: 1.6701789682255004 current Weight -> [2.09079522 3.25091051] ,current bias -> 0.9276744380201063 \n",
      "Epoch: 743 | MAE Train Loss: 1.6622670167920421 current Weight -> [2.09361076 3.25530977] ,current bias -> 0.9285383164347378 \n",
      "Epoch: 744 | MAE Train Loss: 1.6544226227274352 current Weight -> [2.0964263  3.25970909] ,current bias -> 0.9294014084665493 \n",
      "Epoch: 745 | MAE Train Loss: 1.6466457832105714 current Weight -> [2.09924184 3.26410845] ,current bias -> 0.9302637149284912 \n",
      "Epoch: 746 | MAE Train Loss: 1.6389364954274293 current Weight -> [2.10205739 3.26850786] ,current bias -> 0.9311252366326741 \n",
      "Epoch: 747 | MAE Train Loss: 1.6312947565710658 current Weight -> [2.10487294 3.27290732] ,current bias -> 0.9319859743903685 \n",
      "Epoch: 748 | MAE Train Loss: 1.6237205638415906 current Weight -> [2.1076885  3.27730683] ,current bias -> 0.9328459290120064 \n",
      "Epoch: 749 | MAE Train Loss: 1.6162139144461576 current Weight -> [2.11050405 3.28170639] ,current bias -> 0.933705101307182 \n",
      "Epoch: 750 | MAE Train Loss: 1.6087748055989417 current Weight -> [2.11331961 3.28610599] ,current bias -> 0.9345634920846526 \n",
      "Epoch: 751 | MAE Train Loss: 1.6014032345211264 current Weight -> [2.11613518 3.29050565] ,current bias -> 0.9354211021523394 \n",
      "Epoch: 752 | MAE Train Loss: 1.5940991984408859 current Weight -> [2.11895074 3.29490535] ,current bias -> 0.9362779323173285 \n",
      "Epoch: 753 | MAE Train Loss: 1.5868626945933664 current Weight -> [2.12176632 3.2993051 ] ,current bias -> 0.9371339833858713 \n",
      "Epoch: 754 | MAE Train Loss: 1.579693720220673 current Weight -> [2.12458189 3.3037049 ] ,current bias -> 0.9379892561633859 \n",
      "Epoch: 755 | MAE Train Loss: 1.572592272571851 current Weight -> [2.12739746 3.30810475] ,current bias -> 0.938843751454458 \n",
      "Epoch: 756 | MAE Train Loss: 1.5655583489028713 current Weight -> [2.13021304 3.31250465] ,current bias -> 0.9396974700628411 \n",
      "Epoch: 757 | MAE Train Loss: 1.5585919464766098 current Weight -> [2.13302863 3.31690459] ,current bias -> 0.9405504127914581 \n",
      "Epoch: 758 | MAE Train Loss: 1.5516930625628367 current Weight -> [2.13584421 3.32130458] ,current bias -> 0.9414025804424018 \n",
      "Epoch: 759 | MAE Train Loss: 1.5448616944381959 current Weight -> [2.1386598  3.32570462] ,current bias -> 0.9422539738169357 \n",
      "Epoch: 760 | MAE Train Loss: 1.5380978393861908 current Weight -> [2.14147539 3.33010471] ,current bias -> 0.943104593715495 \n",
      "Epoch: 761 | MAE Train Loss: 1.5314014946971652 current Weight -> [2.14429099 3.33450485] ,current bias -> 0.9439544409376877 \n",
      "Epoch: 762 | MAE Train Loss: 1.5247726576682934 current Weight -> [2.14710659 3.33890503] ,current bias -> 0.9448035162822946 \n",
      "Epoch: 763 | MAE Train Loss: 1.5182113256035552 current Weight -> [2.14992219 3.34330526] ,current bias -> 0.9456518205472714 \n",
      "Epoch: 764 | MAE Train Loss: 1.5117174958137263 current Weight -> [2.15273779 3.34770554] ,current bias -> 0.9464993545297483 \n",
      "Epoch: 765 | MAE Train Loss: 1.505291165616361 current Weight -> [2.1555534  3.35210587] ,current bias -> 0.9473461190260318 \n",
      "Epoch: 766 | MAE Train Loss: 1.4989323323357737 current Weight -> [2.15836901 3.35650625] ,current bias -> 0.9481921148316051 \n",
      "Epoch: 767 | MAE Train Loss: 1.4926409933030242 current Weight -> [2.16118462 3.36090667] ,current bias -> 0.949037342741129 \n",
      "Epoch: 768 | MAE Train Loss: 1.486417145855903 current Weight -> [2.16400024 3.36530714] ,current bias -> 0.9498818035484429 \n",
      "Epoch: 769 | MAE Train Loss: 1.4802607873389135 current Weight -> [2.16681586 3.36970766] ,current bias -> 0.9507254980465653 \n",
      "Epoch: 770 | MAE Train Loss: 1.4741719151032562 current Weight -> [2.16963148 3.37410822] ,current bias -> 0.9515684270276951 \n",
      "Epoch: 771 | MAE Train Loss: 1.4681505265068133 current Weight -> [2.17244711 3.37850884] ,current bias -> 0.9524105912832123 \n",
      "Epoch: 772 | MAE Train Loss: 1.4621966189141327 current Weight -> [2.17526274 3.3829095 ] ,current bias -> 0.9532519916036785 \n",
      "Epoch: 773 | MAE Train Loss: 1.456310189696414 current Weight -> [2.17807837 3.3873102 ] ,current bias -> 0.9540926287788383 \n",
      "Epoch: 774 | MAE Train Loss: 1.4504912362314875 current Weight -> [2.18089401 3.39171096] ,current bias -> 0.9549325035976197 \n",
      "Epoch: 775 | MAE Train Loss: 1.4447397559038038 current Weight -> [2.18370964 3.39611176] ,current bias -> 0.9557716168481354 \n",
      "Epoch: 776 | MAE Train Loss: 1.4390557461044162 current Weight -> [2.18652528 3.40051261] ,current bias -> 0.9566099693176829 \n",
      "Epoch: 777 | MAE Train Loss: 1.4334392042309638 current Weight -> [2.18934093 3.40491351] ,current bias -> 0.9574475617927461 \n",
      "Epoch: 778 | MAE Train Loss: 1.427890127687657 current Weight -> [2.19215658 3.40931445] ,current bias -> 0.958284395058996 \n",
      "Epoch: 779 | MAE Train Loss: 1.422408513885263 current Weight -> [2.19497223 3.41371544] ,current bias -> 0.959120469901291 \n",
      "Epoch: 780 | MAE Train Loss: 1.4169943602410866 current Weight -> [2.19778788 3.41811648] ,current bias -> 0.9599557871036782 \n",
      "Epoch: 781 | MAE Train Loss: 1.4116476641789573 current Weight -> [2.20060354 3.42251757] ,current bias -> 0.9607903474493945 \n",
      "Epoch: 782 | MAE Train Loss: 1.406368423129216 current Weight -> [2.2034192 3.4269187] ,current bias -> 0.9616241517208668 \n",
      "Epoch: 783 | MAE Train Loss: 1.4011566345286939 current Weight -> [2.20623486 3.43131988] ,current bias -> 0.9624572006997132 \n",
      "Epoch: 784 | MAE Train Loss: 1.3960122958207017 current Weight -> [2.20905052 3.43572111] ,current bias -> 0.9632894951667438 \n",
      "Epoch: 785 | MAE Train Loss: 1.3909354044550108 current Weight -> [2.21186619 3.44012238] ,current bias -> 0.9641210359019614 \n",
      "Epoch: 786 | MAE Train Loss: 1.3859259578878411 current Weight -> [2.21468187 3.4445237 ] ,current bias -> 0.9649518236845627 \n",
      "Epoch: 787 | MAE Train Loss: 1.380983953581843 current Weight -> [2.21749754 3.44892506] ,current bias -> 0.9657818592929386 \n",
      "Epoch: 788 | MAE Train Loss: 1.3761093890060843 current Weight -> [2.22031322 3.45332648] ,current bias -> 0.9666111435046756 \n",
      "Epoch: 789 | MAE Train Loss: 1.3713022616360322 current Weight -> [2.2231289  3.45772794] ,current bias -> 0.967439677096556 \n",
      "Epoch: 790 | MAE Train Loss: 1.3665625689535423 current Weight -> [2.22594458 3.46212944] ,current bias -> 0.9682674608445593 \n",
      "Epoch: 791 | MAE Train Loss: 1.3618903084468366 current Weight -> [2.22876027 3.46653099] ,current bias -> 0.9690944955238628 \n",
      "Epoch: 792 | MAE Train Loss: 1.357285477610497 current Weight -> [2.23157596 3.47093259] ,current bias -> 0.9699207819088425 \n",
      "Epoch: 793 | MAE Train Loss: 1.3527480739454425 current Weight -> [2.23439165 3.47533424] ,current bias -> 0.9707463207730734 \n",
      "Epoch: 794 | MAE Train Loss: 1.3482780949589173 current Weight -> [2.23720735 3.47973593] ,current bias -> 0.9715711128893315 \n",
      "Epoch: 795 | MAE Train Loss: 1.3438755381644774 current Weight -> [2.24002305 3.48413767] ,current bias -> 0.9723951590295934 \n",
      "Epoch: 796 | MAE Train Loss: 1.3395404010819716 current Weight -> [2.24283875 3.48853945] ,current bias -> 0.9732184599650378 \n",
      "Epoch: 797 | MAE Train Loss: 1.33527268123753 current Weight -> [2.24565445 3.49294128] ,current bias -> 0.9740410164660461 \n",
      "Epoch: 798 | MAE Train Loss: 1.3310723761635466 current Weight -> [2.24847016 3.49734316] ,current bias -> 0.9748628293022035 \n",
      "Epoch: 799 | MAE Train Loss: 1.3269394833986663 current Weight -> [2.25128587 3.50174508] ,current bias -> 0.9756838992422994 \n",
      "Epoch: 800 | MAE Train Loss: 1.3228740004877684 current Weight -> [2.25410159 3.50614705] ,current bias -> 0.9765042270543285 \n",
      "Epoch: 801 | MAE Train Loss: 1.3188759249819524 current Weight -> [2.2569173  3.51054907] ,current bias -> 0.9773238135054916 \n",
      "Epoch: 802 | MAE Train Loss: 1.314945254438523 current Weight -> [2.25973302 3.51495113] ,current bias -> 0.9781426593621962 \n",
      "Epoch: 803 | MAE Train Loss: 1.3110819864209762 current Weight -> [2.26254875 3.51935323] ,current bias -> 0.9789607653900579 \n",
      "Epoch: 804 | MAE Train Loss: 1.3072861184989828 current Weight -> [2.26536447 3.52375539] ,current bias -> 0.9797781323539004 \n",
      "Epoch: 805 | MAE Train Loss: 1.3035576482483748 current Weight -> [2.2681802  3.52815758] ,current bias -> 0.980594761017757 \n",
      "Epoch: 806 | MAE Train Loss: 1.29989657325113 current Weight -> [2.27099593 3.53255983] ,current bias -> 0.981410652144871 \n",
      "Epoch: 807 | MAE Train Loss: 1.2963028910953591 current Weight -> [2.27381167 3.53696212] ,current bias -> 0.9822258064976966 \n",
      "Epoch: 808 | MAE Train Loss: 1.2927765993752882 current Weight -> [2.2766274  3.54136445] ,current bias -> 0.9830402248379001 \n",
      "Epoch: 809 | MAE Train Loss: 1.2893176956912469 current Weight -> [2.27944314 3.54576684] ,current bias -> 0.9838539079263602 \n",
      "Epoch: 810 | MAE Train Loss: 1.2859261776496518 current Weight -> [2.28225889 3.55016926] ,current bias -> 0.9846668565231689 \n",
      "Epoch: 811 | MAE Train Loss: 1.2826020428629916 current Weight -> [2.28507463 3.55457173] ,current bias -> 0.9854790713876326 \n",
      "Epoch: 812 | MAE Train Loss: 1.2793452889498165 current Weight -> [2.28789038 3.55897425] ,current bias -> 0.9862905532782728 \n",
      "Epoch: 813 | MAE Train Loss: 1.2761559135347185 current Weight -> [2.29070614 3.56337682] ,current bias -> 0.9871013029528266 \n",
      "Epoch: 814 | MAE Train Loss: 1.2730339142483202 current Weight -> [2.29352189 3.56777942] ,current bias -> 0.9879113211682481 \n",
      "Epoch: 815 | MAE Train Loss: 1.269979288727259 current Weight -> [2.29633765 3.57218208] ,current bias -> 0.9887206086807085 \n",
      "Epoch: 816 | MAE Train Loss: 1.2669920346141739 current Weight -> [2.29915341 3.57658478] ,current bias -> 0.9895291662455976 \n",
      "Epoch: 817 | MAE Train Loss: 1.2640721495576899 current Weight -> [2.30196917 3.58098752] ,current bias -> 0.990336994617524 \n",
      "Epoch: 818 | MAE Train Loss: 1.2612196312124049 current Weight -> [2.30478494 3.58539031] ,current bias -> 0.9911440945503165 \n",
      "Epoch: 819 | MAE Train Loss: 1.2584344772388736 current Weight -> [2.30760071 3.58979315] ,current bias -> 0.9919504667970244 \n",
      "Epoch: 820 | MAE Train Loss: 1.2557166853035955 current Weight -> [2.31041648 3.59419603] ,current bias -> 0.9927561121099185 \n",
      "Epoch: 821 | MAE Train Loss: 1.2530662530789989 current Weight -> [2.31323226 3.59859895] ,current bias -> 0.9935610312404921 \n",
      "Epoch: 822 | MAE Train Loss: 1.2504831782434278 current Weight -> [2.31604804 3.60300192] ,current bias -> 0.9943652249394614 \n",
      "Epoch: 823 | MAE Train Loss: 1.247967458481128 current Weight -> [2.31886382 3.60740494] ,current bias -> 0.9951686939567668 \n",
      "Epoch: 824 | MAE Train Loss: 1.2455190914822294 current Weight -> [2.32167961 3.611808  ] ,current bias -> 0.9959714390415729 \n",
      "Epoch: 825 | MAE Train Loss: 1.2431380749427388 current Weight -> [2.32449539 3.61621111] ,current bias -> 0.9967734609422705 \n",
      "Epoch: 826 | MAE Train Loss: 1.2408244065645189 current Weight -> [2.32731118 3.62061426] ,current bias -> 0.9975747604064762 \n",
      "Epoch: 827 | MAE Train Loss: 1.2385780840552785 current Weight -> [2.33012698 3.62501745] ,current bias -> 0.998375338181034 \n",
      "Epoch: 828 | MAE Train Loss: 1.236399105128557 current Weight -> [2.33294277 3.62942069] ,current bias -> 0.9991751950120156 \n",
      "Epoch: 829 | MAE Train Loss: 1.2342874675037103 current Weight -> [2.33575857 3.63382397] ,current bias -> 0.9999743316447217 \n",
      "Epoch: 830 | MAE Train Loss: 1.2322431689058977 current Weight -> [2.33857437 3.6382273 ] ,current bias -> 1.0007727488236824 \n",
      "Epoch: 831 | MAE Train Loss: 1.230266207066067 current Weight -> [2.34139018 3.64263068] ,current bias -> 1.001570447292658 \n",
      "Epoch: 832 | MAE Train Loss: 1.228356579720942 current Weight -> [2.34420599 3.6470341 ] ,current bias -> 1.0023674277946402 \n",
      "Epoch: 833 | MAE Train Loss: 1.226514284613007 current Weight -> [2.3470218  3.65143756] ,current bias -> 1.003163691071852 \n",
      "Epoch: 834 | MAE Train Loss: 1.2247393194904936 current Weight -> [2.34983761 3.65584107] ,current bias -> 1.0039592378657498 \n",
      "Epoch: 835 | MAE Train Loss: 1.2230316821073677 current Weight -> [2.35265343 3.66024462] ,current bias -> 1.0047540689170233 \n",
      "Epoch: 836 | MAE Train Loss: 1.221391370223315 current Weight -> [2.35546925 3.66464821] ,current bias -> 1.0055481849655963 \n",
      "Epoch: 837 | MAE Train Loss: 1.2198183816037274 current Weight -> [2.35828507 3.66905185] ,current bias -> 1.0063415867506278 \n",
      "Epoch: 838 | MAE Train Loss: 1.218312714019689 current Weight -> [2.36110089 3.67345554] ,current bias -> 1.0071342750105126 \n",
      "Epoch: 839 | MAE Train Loss: 1.2168743652479632 current Weight -> [2.36391672 3.67785927] ,current bias -> 1.007926250482882 \n",
      "Epoch: 840 | MAE Train Loss: 1.2155033330709792 current Weight -> [2.36673255 3.68226304] ,current bias -> 1.0087175139046054 \n",
      "Epoch: 841 | MAE Train Loss: 1.2141996152768169 current Weight -> [2.36954839 3.68666686] ,current bias -> 1.0095080660117897 \n",
      "Epoch: 842 | MAE Train Loss: 1.2129632096591945 current Weight -> [2.37236422 3.69107072] ,current bias -> 1.010297907539781 \n",
      "Epoch: 843 | MAE Train Loss: 1.2117941140174553 current Weight -> [2.37518006 3.69547462] ,current bias -> 1.0110870392231657 \n",
      "Epoch: 844 | MAE Train Loss: 1.2106923261565536 current Weight -> [2.3779959  3.69987857] ,current bias -> 1.01187546179577 \n",
      "Epoch: 845 | MAE Train Loss: 1.2096578438870411 current Weight -> [2.38081175 3.70428257] ,current bias -> 1.0126631759906621 \n",
      "Epoch: 846 | MAE Train Loss: 1.2086906650250533 current Weight -> [2.3836276 3.7086866] ,current bias -> 1.0134501825401523 \n",
      "Epoch: 847 | MAE Train Loss: 1.2077907873922964 current Weight -> [2.38644345 3.71309069] ,current bias -> 1.0142364821757934 \n",
      "Epoch: 848 | MAE Train Loss: 1.2069582088160367 current Weight -> [2.3892593  3.71749481] ,current bias -> 1.0150220756283825 \n",
      "Epoch: 849 | MAE Train Loss: 1.20619292712908 current Weight -> [2.39207516 3.72189898] ,current bias -> 1.0158069636279607 \n",
      "Epoch: 850 | MAE Train Loss: 1.2054949401697668 current Weight -> [2.39489102 3.72630319] ,current bias -> 1.016591146903815 \n",
      "Epoch: 851 | MAE Train Loss: 1.204864245781953 current Weight -> [2.39770688 3.73070745] ,current bias -> 1.0173746261844776 \n",
      "Epoch: 852 | MAE Train Loss: 1.2043008418149992 current Weight -> [2.40052274 3.73511175] ,current bias -> 1.0181574021977287 \n",
      "Epoch: 853 | MAE Train Loss: 1.2038047261237566 current Weight -> [2.40333861 3.73951609] ,current bias -> 1.018939475670595 \n",
      "Epoch: 854 | MAE Train Loss: 1.2033758965685553 current Weight -> [2.40615448 3.74392048] ,current bias -> 1.0197208473293524 \n",
      "Epoch: 855 | MAE Train Loss: 1.203014351015188 current Weight -> [2.40897036 3.74832491] ,current bias -> 1.0205015178995256 \n",
      "Epoch: 856 | MAE Train Loss: 1.202720087334902 current Weight -> [2.41178623 3.75272939] ,current bias -> 1.0212814881058894 \n",
      "Epoch: 857 | MAE Train Loss: 1.2024931034043793 current Weight -> [2.41460211 3.7571339 ] ,current bias -> 1.0220607586724693 \n",
      "Epoch: 858 | MAE Train Loss: 1.20233339710573 current Weight -> [2.41741799 3.76153846] ,current bias -> 1.0228393303225423 \n",
      "Epoch: 859 | MAE Train Loss: 1.2022409663264753 current Weight -> [2.42023388 3.76594307] ,current bias -> 1.0236172037786377 \n",
      "Epoch: 860 | MAE Train Loss: 1.2022158089595347 current Weight -> [2.42304976 3.77034772] ,current bias -> 1.024394379762538 \n",
      "Epoch: 861 | MAE Train Loss: 1.2022579229032164 current Weight -> [2.42586565 3.77475241] ,current bias -> 1.0251708589952795 \n",
      "Epoch: 862 | MAE Train Loss: 1.2023673060612001 current Weight -> [2.42868155 3.77915714] ,current bias -> 1.0259466421971528 \n",
      "Epoch: 863 | MAE Train Loss: 1.2025439563425255 current Weight -> [2.43149744 3.78356192] ,current bias -> 1.026721730087704 \n",
      "Epoch: 864 | MAE Train Loss: 1.202787871661581 current Weight -> [2.43431334 3.78796674] ,current bias -> 1.0274961233857356 \n",
      "Epoch: 865 | MAE Train Loss: 1.2030990499380898 current Weight -> [2.43712924 3.7923716 ] ,current bias -> 1.0282698228093068 \n",
      "Epoch: 866 | MAE Train Loss: 1.2034774890970952 current Weight -> [2.43994515 3.79677651] ,current bias -> 1.0290428290757345 \n",
      "Epoch: 867 | MAE Train Loss: 1.2039231870689502 current Weight -> [2.44276105 3.80118146] ,current bias -> 1.029815142901594 \n",
      "Epoch: 868 | MAE Train Loss: 1.2044361417893048 current Weight -> [2.44557696 3.80558645] ,current bias -> 1.03058676500272 \n",
      "Epoch: 869 | MAE Train Loss: 1.2050163511990908 current Weight -> [2.44839288 3.80999149] ,current bias -> 1.031357696094207 \n",
      "Epoch: 870 | MAE Train Loss: 1.2056638132445123 current Weight -> [2.45120879 3.81439656] ,current bias -> 1.0321279368904102 \n",
      "Epoch: 871 | MAE Train Loss: 1.206378525877031 current Weight -> [2.45402471 3.81880169] ,current bias -> 1.0328974881049466 \n",
      "Epoch: 872 | MAE Train Loss: 1.2071604870533528 current Weight -> [2.45684063 3.82320685] ,current bias -> 1.0336663504506953 \n",
      "Epoch: 873 | MAE Train Loss: 1.2080096947354175 current Weight -> [2.45965656 3.82761206] ,current bias -> 1.0344345246397983 \n",
      "Epoch: 874 | MAE Train Loss: 1.208926146890384 current Weight -> [2.46247248 3.83201731] ,current bias -> 1.0352020113836615 \n",
      "Epoch: 875 | MAE Train Loss: 1.2099098414906198 current Weight -> [2.46528841 3.8364226 ] ,current bias -> 1.0359688113929555 \n",
      "Epoch: 876 | MAE Train Loss: 1.2109607765136865 current Weight -> [2.46810434 3.84082793] ,current bias -> 1.0367349253776161 \n",
      "Epoch: 877 | MAE Train Loss: 1.2120789499423275 current Weight -> [2.47092028 3.84523331] ,current bias -> 1.0375003540468453 \n",
      "Epoch: 878 | MAE Train Loss: 1.213264359764458 current Weight -> [2.47373621 3.84963873] ,current bias -> 1.0382650981091115 \n",
      "Epoch: 879 | MAE Train Loss: 1.214517003973149 current Weight -> [2.47655216 3.85404419] ,current bias -> 1.0390291582721514 \n",
      "Epoch: 880 | MAE Train Loss: 1.215836880566617 current Weight -> [2.4793681  3.85844969] ,current bias -> 1.0397925352429693 \n",
      "Epoch: 881 | MAE Train Loss: 1.2172239875482123 current Weight -> [2.48218404 3.86285524] ,current bias -> 1.0405552297278393 \n",
      "Epoch: 882 | MAE Train Loss: 1.2186783229264027 current Weight -> [2.48499999 3.86726083] ,current bias -> 1.041317242432305 \n",
      "Epoch: 883 | MAE Train Loss: 1.2201998847147664 current Weight -> [2.48781594 3.87166646] ,current bias -> 1.0420785740611804 \n",
      "Epoch: 884 | MAE Train Loss: 1.2217886709319763 current Weight -> [2.4906319  3.87607214] ,current bias -> 1.0428392253185514 \n",
      "Epoch: 885 | MAE Train Loss: 1.2234446796017884 current Weight -> [2.49344785 3.88047785] ,current bias -> 1.0435991969077758 \n",
      "Epoch: 886 | MAE Train Loss: 1.225167908753029 current Weight -> [2.49626381 3.88488361] ,current bias -> 1.0443584895314841 \n",
      "Epoch: 887 | MAE Train Loss: 1.2269583564195843 current Weight -> [2.49907978 3.88928941] ,current bias -> 1.0451171038915805 \n",
      "Epoch: 888 | MAE Train Loss: 1.2288160206403875 current Weight -> [2.50189574 3.89369525] ,current bias -> 1.0458750406892439 \n",
      "Epoch: 889 | MAE Train Loss: 1.2307408994594042 current Weight -> [2.50471171 3.89810114] ,current bias -> 1.0466323006249278 \n",
      "Epoch: 890 | MAE Train Loss: 1.2327329909256237 current Weight -> [2.50752768 3.90250706] ,current bias -> 1.047388884398362 \n",
      "Epoch: 891 | MAE Train Loss: 1.234792293093045 current Weight -> [2.51034365 3.90691303] ,current bias -> 1.048144792708553 \n",
      "Epoch: 892 | MAE Train Loss: 1.2369188040206647 current Weight -> [2.51315963 3.91131904] ,current bias -> 1.0489000262537842 \n",
      "Epoch: 893 | MAE Train Loss: 1.2391125217724652 current Weight -> [2.51597561 3.91572509] ,current bias -> 1.0496545857316177 \n",
      "Epoch: 894 | MAE Train Loss: 1.2413734444174032 current Weight -> [2.51879159 3.92013118] ,current bias -> 1.050408471838894 \n",
      "Epoch: 895 | MAE Train Loss: 1.2437015700293979 current Weight -> [2.52160757 3.92453732] ,current bias -> 1.0511616852717334 \n",
      "Epoch: 896 | MAE Train Loss: 1.2460968966873163 current Weight -> [2.52442356 3.9289435 ] ,current bias -> 1.0519142267255366 \n",
      "Epoch: 897 | MAE Train Loss: 1.2485594224749645 current Weight -> [2.52723955 3.93334972] ,current bias -> 1.0526660968949852 \n",
      "Epoch: 898 | MAE Train Loss: 1.2510891454810749 current Weight -> [2.53005554 3.93775598] ,current bias -> 1.053417296474043 \n",
      "Epoch: 899 | MAE Train Loss: 1.2536860637992941 current Weight -> [2.53287153 3.94216228] ,current bias -> 1.054167826155956 \n"
     ]
    }
   ],
   "source": [
    "tol = 1e-3\n",
    "epochs = 900\n",
    "learning_rate = 0.001\n",
    "\n",
    "GradientDescent(X,y,epochs,learning_rate,tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
